{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "ffcb0ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a938bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/nguyenviet/Desktop/CSC311/group-project')  # Change to project root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c5a2b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/train/features_train_with_bow.csv', header=None)\n",
    "y_train_one = pd.read_csv('data/train/targets_train_one_hot.csv', header=None)\n",
    "y_train = pd.read_csv('data/train/targets_train_enc.csv', header=None)\n",
    "\n",
    "X_valid = pd.read_csv('data/valid/features_valid_with_bow.csv', header=None)\n",
    "y_valid_one = pd.read_csv('data/valid/targets_valid_one_hot.csv', header=None)\n",
    "y_valid = pd.read_csv('data/valid/targets_valid_enc.csv', header=None)\n",
    "\n",
    "X_test = pd.read_csv('data/test/features_test_with_bow.csv', header=None)\n",
    "y_test_one = pd.read_csv('data/test/targets_test_one_hot.csv', header=None)\n",
    "y_test = pd.read_csv('data/test/targets_test_enc.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ac994cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_one\n",
    "np.array(X_train)\n",
    "np.int64(np.ones(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "0c32787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.insert(0, 'bias', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143c545",
   "metadata": {},
   "source": [
    "# Softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "0bcd202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z_max = np.max(z, axis=1, keepdims=True)\n",
    "    e_z = np.exp(z - z_max)\n",
    "    return e_z / np.sum(e_z, axis=1, keepdims=True)\n",
    "\n",
    "def predict(X, W, b):\n",
    "    logits = X @ W + b\n",
    "    probs = softmax(logits)\n",
    "    return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe80e7",
   "metadata": {},
   "source": [
    "# Loss function (Cross-Entropy Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "def8f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_entropy_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Calculates the cross-entropy loss.\n",
    "#     y_true: One-hot encoded true labels.\n",
    "#     y_pred: Predicted probabilities from softmax.\n",
    "#     \"\"\"\n",
    "#     m = y_true.shape[0]\n",
    "#     # Avoid log(0) by clipping predicted probabilities\n",
    "#     y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "#     loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "#     return loss\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred, W=None, lambda_reg=0):\n",
    "    \"\"\"\n",
    "    Extended to include regularization term\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "    data_loss = -np.sum(y_true * np.log(y_pred)) / m\n",
    "    \n",
    "    # Add regularization loss if weights are provided\n",
    "    if W is not None and lambda_reg > 0:\n",
    "        reg_loss = (lambda_reg / 2) * np.sum(W * W)\n",
    "        return data_loss + reg_loss\n",
    "    else:\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e828552e",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5d428b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fit_softmax_regression(X, y_one_hot, learning_rate, num_iterations):\n",
    "#     np.random.seed(42)\n",
    "#     X = np.asarray(X)          # safer, works with DataFrame or ndarray\n",
    "#     y_one_hot = np.asarray(y_one_hot)\n",
    "#     n_samples, n_features = X.shape\n",
    "#     n_classes = y_one_hot.shape[1]\n",
    "\n",
    "#     W = np.random.randn(n_features, n_classes) * 0.01\n",
    "#     b = np.zeros(n_classes)           # ← this is the missing bias\n",
    "\n",
    "#     for i in range(num_iterations):\n",
    "#         logits = X @ W + b            # ← add bias here\n",
    "#         y_pred = softmax(logits)\n",
    "\n",
    "#         if i % 100 == 0:\n",
    "#             loss = cross_entropy_loss(y_one_hot, y_pred)\n",
    "#             print(f\"Iteration {i}, Loss: {loss:.6f}\")\n",
    "\n",
    "#         dW = X.T @ (y_pred - y_one_hot) / n_samples\n",
    "#         db = np.sum(y_pred - y_one_hot, axis=0) / n_samples   # ← bias gradient\n",
    "\n",
    "#         W -= learning_rate * dW\n",
    "#         b -= learning_rate * db\n",
    "\n",
    "#     return W, b\n",
    "\n",
    "def fit_softmax_regression(X, y_one_hot, learning_rate, num_iterations, C=1.0):\n",
    "    \"\"\"\n",
    "    C: Inverse of L2 regularization strength\n",
    "    - Smaller C = stronger regularization\n",
    "    - Larger C = weaker regularization\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    y_one_hot = np.asarray(y_one_hot)\n",
    "    n_samples, n_features = X.shape\n",
    "    n_classes = y_one_hot.shape[1]\n",
    "\n",
    "    W = np.random.randn(n_features, n_classes) * 0.01\n",
    "    b = np.zeros(n_classes)\n",
    "\n",
    "    # Convert C to lambda (regularization strength)\n",
    "    # C = 1/lambda, so lambda = 1/C\n",
    "    lambda_reg = 1.0 / C if C > 0 else 0\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        logits = X @ W + b\n",
    "        y_pred = softmax(logits)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # Add regularization to loss calculation\n",
    "            data_loss = cross_entropy_loss(y_one_hot, y_pred)\n",
    "            reg_loss = (lambda_reg / 2) * np.sum(W * W)  # L2 regularization term\n",
    "            total_loss = data_loss + reg_loss\n",
    "            print(f\"Iteration {i}, Loss: {total_loss:.6f} (data: {data_loss:.6f}, reg: {reg_loss:.6f})\")\n",
    "\n",
    "        # Add regularization to gradient\n",
    "        dW = (X.T @ (y_pred - y_one_hot)) / n_samples + lambda_reg * W\n",
    "        db = np.sum(y_pred - y_one_hot, axis=0) / n_samples\n",
    "\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "386d3edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X_train, y_train_one, X_valid, y_valid_one):\n",
    "    learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "    num_iterations_list = [500, 1000, 2000]\n",
    "    C_values = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]  # Inverse L2 regularization strength\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for num_iter in num_iterations_list:\n",
    "            for C in C_values:\n",
    "                W, b = fit_softmax_regression(X_train.values, y_train_one.values, \n",
    "                                             learning_rate=lr, num_iterations=num_iter, C=C)\n",
    "                val_pred = predict(X_valid.values, W, b)\n",
    "                val_acc = np.mean(val_pred == y_valid.values.ravel())\n",
    "\n",
    "                print(f\"LR: {lr}, Iter: {num_iter}, C: {C}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_params = {'learning_rate': lr, 'num_iterations': num_iter, 'C': C}\n",
    "\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.4f} with params: {best_params}\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "668bdc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 229.981088 (data: 1.094060, reg: 228.887028)\n",
      "Iteration 100, Loss: 1.098169 (data: 1.097973, reg: 0.000196)\n",
      "Iteration 200, Loss: 1.097939 (data: 1.097744, reg: 0.000195)\n",
      "Iteration 300, Loss: 1.097723 (data: 1.097530, reg: 0.000194)\n",
      "Iteration 400, Loss: 1.097522 (data: 1.097329, reg: 0.000193)\n",
      "LR: 0.001, Iter: 500, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 24.336271 (data: 1.104655, reg: 23.231616)\n",
      "Iteration 100, Loss: 1.096432 (data: 1.094505, reg: 0.001927)\n",
      "Iteration 200, Loss: 1.096221 (data: 1.094304, reg: 0.001917)\n",
      "Iteration 300, Loss: 1.096022 (data: 1.094115, reg: 0.001907)\n",
      "Iteration 400, Loss: 1.095837 (data: 1.093939, reg: 0.001897)\n",
      "LR: 0.001, Iter: 500, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.399387 (data: 1.094307, reg: 2.305080)\n",
      "Iteration 100, Loss: 1.390478 (data: 1.073561, reg: 0.316917)\n",
      "Iteration 200, Loss: 1.121749 (data: 1.066936, reg: 0.054813)\n",
      "Iteration 300, Loss: 1.085733 (data: 1.064707, reg: 0.021026)\n",
      "Iteration 400, Loss: 1.080848 (data: 1.063928, reg: 0.016920)\n",
      "LR: 0.001, Iter: 500, C: 0.1, Val Acc: 0.4966\n",
      "Iteration 0, Loss: 1.332740 (data: 1.101238, reg: 0.231502)\n",
      "Iteration 100, Loss: 1.257760 (data: 1.066980, reg: 0.190779)\n",
      "Iteration 200, Loss: 1.200352 (data: 1.040412, reg: 0.159941)\n",
      "Iteration 300, Loss: 1.155746 (data: 1.019208, reg: 0.136538)\n",
      "Iteration 400, Loss: 1.120711 (data: 1.001913, reg: 0.118799)\n",
      "LR: 0.001, Iter: 500, C: 1.0, Val Acc: 0.5436\n",
      "Iteration 0, Loss: 1.124190 (data: 1.101992, reg: 0.022198)\n",
      "Iteration 100, Loss: 1.086972 (data: 1.065073, reg: 0.021898)\n",
      "Iteration 200, Loss: 1.056396 (data: 1.034485, reg: 0.021910)\n",
      "Iteration 300, Loss: 1.030556 (data: 1.008398, reg: 0.022158)\n",
      "Iteration 400, Loss: 1.008197 (data: 0.985609, reg: 0.022588)\n",
      "LR: 0.001, Iter: 500, C: 10.0, Val Acc: 0.5772\n",
      "Iteration 0, Loss: 1.099398 (data: 1.097063, reg: 0.002336)\n",
      "Iteration 100, Loss: 1.062759 (data: 1.060409, reg: 0.002350)\n",
      "Iteration 200, Loss: 1.032061 (data: 1.029667, reg: 0.002394)\n",
      "Iteration 300, Loss: 1.005635 (data: 1.003173, reg: 0.002463)\n",
      "Iteration 400, Loss: 0.982370 (data: 0.979819, reg: 0.002550)\n",
      "LR: 0.001, Iter: 500, C: 100.0, Val Acc: 0.5503\n",
      "Iteration 0, Loss: 228.359269 (data: 1.102935, reg: 227.256334)\n",
      "Iteration 100, Loss: 1.098169 (data: 1.097974, reg: 0.000196)\n",
      "Iteration 200, Loss: 1.097939 (data: 1.097745, reg: 0.000195)\n",
      "Iteration 300, Loss: 1.097724 (data: 1.097530, reg: 0.000194)\n",
      "Iteration 400, Loss: 1.097522 (data: 1.097330, reg: 0.000193)\n",
      "Iteration 500, Loss: 1.097334 (data: 1.097142, reg: 0.000192)\n",
      "Iteration 600, Loss: 1.097158 (data: 1.096967, reg: 0.000191)\n",
      "Iteration 700, Loss: 1.096993 (data: 1.096803, reg: 0.000190)\n",
      "Iteration 800, Loss: 1.096839 (data: 1.096650, reg: 0.000189)\n",
      "Iteration 900, Loss: 1.096695 (data: 1.096507, reg: 0.000188)\n",
      "LR: 0.001, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.746604 (data: 1.103166, reg: 22.643438)\n",
      "Iteration 100, Loss: 1.096434 (data: 1.094507, reg: 0.001927)\n",
      "Iteration 200, Loss: 1.096222 (data: 1.094306, reg: 0.001917)\n",
      "Iteration 300, Loss: 1.096024 (data: 1.094117, reg: 0.001907)\n",
      "Iteration 400, Loss: 1.095838 (data: 1.093941, reg: 0.001897)\n",
      "Iteration 500, Loss: 1.095664 (data: 1.093776, reg: 0.001888)\n",
      "Iteration 600, Loss: 1.095501 (data: 1.093622, reg: 0.001880)\n",
      "Iteration 700, Loss: 1.095349 (data: 1.093478, reg: 0.001871)\n",
      "Iteration 800, Loss: 1.095206 (data: 1.093343, reg: 0.001863)\n",
      "Iteration 900, Loss: 1.095073 (data: 1.093217, reg: 0.001855)\n",
      "LR: 0.001, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.369004 (data: 1.094050, reg: 2.274955)\n",
      "Iteration 100, Loss: 1.386302 (data: 1.073262, reg: 0.313039)\n",
      "Iteration 200, Loss: 1.121182 (data: 1.066782, reg: 0.054399)\n",
      "Iteration 300, Loss: 1.085660 (data: 1.064640, reg: 0.021020)\n",
      "Iteration 400, Loss: 1.080842 (data: 1.063901, reg: 0.016941)\n",
      "Iteration 500, Loss: 1.080134 (data: 1.063631, reg: 0.016503)\n",
      "Iteration 600, Loss: 1.079979 (data: 1.063521, reg: 0.016458)\n",
      "Iteration 700, Loss: 1.079902 (data: 1.063466, reg: 0.016436)\n",
      "Iteration 800, Loss: 1.079838 (data: 1.063432, reg: 0.016406)\n",
      "Iteration 900, Loss: 1.079779 (data: 1.063406, reg: 0.016373)\n",
      "LR: 0.001, Iter: 1000, C: 0.1, Val Acc: 0.4698\n",
      "Iteration 0, Loss: 1.333833 (data: 1.109077, reg: 0.224756)\n",
      "Iteration 100, Loss: 1.256833 (data: 1.072109, reg: 0.184724)\n",
      "Iteration 200, Loss: 1.198579 (data: 1.043891, reg: 0.154688)\n",
      "Iteration 300, Loss: 1.153719 (data: 1.021648, reg: 0.132072)\n",
      "Iteration 400, Loss: 1.118715 (data: 1.003667, reg: 0.115048)\n",
      "Iteration 500, Loss: 1.091142 (data: 0.988860, reg: 0.102282)\n",
      "Iteration 600, Loss: 1.069281 (data: 0.976504, reg: 0.092777)\n",
      "Iteration 700, Loss: 1.051865 (data: 0.966091, reg: 0.085774)\n",
      "Iteration 800, Loss: 1.037943 (data: 0.957251, reg: 0.080691)\n",
      "Iteration 900, Loss: 1.026784 (data: 0.949705, reg: 0.077079)\n",
      "LR: 0.001, Iter: 1000, C: 1.0, Val Acc: 0.5973\n",
      "Iteration 0, Loss: 1.113006 (data: 1.089699, reg: 0.023307)\n",
      "Iteration 100, Loss: 1.078274 (data: 1.055186, reg: 0.023087)\n",
      "Iteration 200, Loss: 1.049420 (data: 1.026269, reg: 0.023151)\n",
      "Iteration 300, Loss: 1.024799 (data: 1.001369, reg: 0.023430)\n",
      "Iteration 400, Loss: 1.003335 (data: 0.979454, reg: 0.023881)\n",
      "Iteration 500, Loss: 0.984315 (data: 0.959845, reg: 0.024470)\n",
      "Iteration 600, Loss: 0.967261 (data: 0.942087, reg: 0.025174)\n",
      "Iteration 700, Loss: 0.951837 (data: 0.925862, reg: 0.025974)\n",
      "Iteration 800, Loss: 0.937795 (data: 0.910939, reg: 0.026856)\n",
      "Iteration 900, Loss: 0.924949 (data: 0.897142, reg: 0.027808)\n",
      "LR: 0.001, Iter: 1000, C: 10.0, Val Acc: 0.6309\n",
      "Iteration 0, Loss: 1.104805 (data: 1.102552, reg: 0.002253)\n",
      "Iteration 100, Loss: 1.067970 (data: 1.065709, reg: 0.002261)\n",
      "Iteration 200, Loss: 1.037048 (data: 1.034747, reg: 0.002301)\n",
      "Iteration 300, Loss: 1.010374 (data: 1.008009, reg: 0.002365)\n",
      "Iteration 400, Loss: 0.986850 (data: 0.984402, reg: 0.002448)\n",
      "Iteration 500, Loss: 0.965754 (data: 0.963206, reg: 0.002548)\n",
      "Iteration 600, Loss: 0.946599 (data: 0.943938, reg: 0.002661)\n",
      "Iteration 700, Loss: 0.929048 (data: 0.926262, reg: 0.002786)\n",
      "Iteration 800, Loss: 0.912856 (data: 0.909934, reg: 0.002922)\n",
      "Iteration 900, Loss: 0.897838 (data: 0.894771, reg: 0.003067)\n",
      "LR: 0.001, Iter: 1000, C: 100.0, Val Acc: 0.6107\n",
      "Iteration 0, Loss: 220.887837 (data: 1.093376, reg: 219.794461)\n",
      "Iteration 100, Loss: 1.098169 (data: 1.097973, reg: 0.000196)\n",
      "Iteration 200, Loss: 1.097939 (data: 1.097744, reg: 0.000195)\n",
      "Iteration 300, Loss: 1.097723 (data: 1.097530, reg: 0.000194)\n",
      "Iteration 400, Loss: 1.097522 (data: 1.097329, reg: 0.000193)\n",
      "Iteration 500, Loss: 1.097334 (data: 1.097142, reg: 0.000192)\n",
      "Iteration 600, Loss: 1.097157 (data: 1.096967, reg: 0.000191)\n",
      "Iteration 700, Loss: 1.096993 (data: 1.096803, reg: 0.000190)\n",
      "Iteration 800, Loss: 1.096839 (data: 1.096650, reg: 0.000189)\n",
      "Iteration 900, Loss: 1.096695 (data: 1.096507, reg: 0.000188)\n",
      "Iteration 1000, Loss: 1.096560 (data: 1.096373, reg: 0.000187)\n",
      "Iteration 1100, Loss: 1.096435 (data: 1.096248, reg: 0.000187)\n",
      "Iteration 1200, Loss: 1.096317 (data: 1.096131, reg: 0.000186)\n",
      "Iteration 1300, Loss: 1.096207 (data: 1.096022, reg: 0.000185)\n",
      "Iteration 1400, Loss: 1.096105 (data: 1.095920, reg: 0.000184)\n",
      "Iteration 1500, Loss: 1.096009 (data: 1.095825, reg: 0.000184)\n",
      "Iteration 1600, Loss: 1.095920 (data: 1.095736, reg: 0.000183)\n",
      "Iteration 1700, Loss: 1.095836 (data: 1.095653, reg: 0.000183)\n",
      "Iteration 1800, Loss: 1.095758 (data: 1.095576, reg: 0.000182)\n",
      "Iteration 1900, Loss: 1.095685 (data: 1.095503, reg: 0.000182)\n",
      "LR: 0.001, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.724154 (data: 1.096264, reg: 22.627890)\n",
      "Iteration 100, Loss: 1.096427 (data: 1.094500, reg: 0.001927)\n",
      "Iteration 200, Loss: 1.096216 (data: 1.094299, reg: 0.001917)\n",
      "Iteration 300, Loss: 1.096018 (data: 1.094111, reg: 0.001907)\n",
      "Iteration 400, Loss: 1.095832 (data: 1.093935, reg: 0.001897)\n",
      "Iteration 500, Loss: 1.095659 (data: 1.093771, reg: 0.001888)\n",
      "Iteration 600, Loss: 1.095496 (data: 1.093617, reg: 0.001879)\n",
      "Iteration 700, Loss: 1.095344 (data: 1.093473, reg: 0.001871)\n",
      "Iteration 800, Loss: 1.095202 (data: 1.093339, reg: 0.001863)\n",
      "Iteration 900, Loss: 1.095069 (data: 1.093213, reg: 0.001855)\n",
      "Iteration 1000, Loss: 1.094944 (data: 1.093096, reg: 0.001848)\n",
      "Iteration 1100, Loss: 1.094828 (data: 1.092987, reg: 0.001841)\n",
      "Iteration 1200, Loss: 1.094718 (data: 1.092884, reg: 0.001834)\n",
      "Iteration 1300, Loss: 1.094616 (data: 1.092789, reg: 0.001828)\n",
      "Iteration 1400, Loss: 1.094521 (data: 1.092699, reg: 0.001822)\n",
      "Iteration 1500, Loss: 1.094432 (data: 1.092616, reg: 0.001816)\n",
      "Iteration 1600, Loss: 1.094348 (data: 1.092538, reg: 0.001810)\n",
      "Iteration 1700, Loss: 1.094270 (data: 1.092465, reg: 0.001805)\n",
      "Iteration 1800, Loss: 1.094197 (data: 1.092397, reg: 0.001799)\n",
      "Iteration 1900, Loss: 1.094129 (data: 1.092334, reg: 0.001795)\n",
      "LR: 0.001, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.372051 (data: 1.104814, reg: 2.267237)\n",
      "Iteration 100, Loss: 1.386519 (data: 1.076772, reg: 0.309747)\n",
      "Iteration 200, Loss: 1.121184 (data: 1.067983, reg: 0.053201)\n",
      "Iteration 300, Loss: 1.085645 (data: 1.065055, reg: 0.020590)\n",
      "Iteration 400, Loss: 1.080826 (data: 1.064043, reg: 0.016783)\n",
      "Iteration 500, Loss: 1.080119 (data: 1.063678, reg: 0.016441)\n",
      "Iteration 600, Loss: 1.079965 (data: 1.063534, reg: 0.016431)\n",
      "Iteration 700, Loss: 1.079889 (data: 1.063468, reg: 0.016421)\n",
      "Iteration 800, Loss: 1.079826 (data: 1.063430, reg: 0.016396)\n",
      "Iteration 900, Loss: 1.079767 (data: 1.063403, reg: 0.016365)\n",
      "Iteration 1000, Loss: 1.079712 (data: 1.063381, reg: 0.016332)\n",
      "Iteration 1100, Loss: 1.079660 (data: 1.063361, reg: 0.016299)\n",
      "Iteration 1200, Loss: 1.079611 (data: 1.063344, reg: 0.016267)\n",
      "Iteration 1300, Loss: 1.079565 (data: 1.063329, reg: 0.016235)\n",
      "Iteration 1400, Loss: 1.079521 (data: 1.063315, reg: 0.016205)\n",
      "Iteration 1500, Loss: 1.079479 (data: 1.063303, reg: 0.016176)\n",
      "Iteration 1600, Loss: 1.079440 (data: 1.063292, reg: 0.016148)\n",
      "Iteration 1700, Loss: 1.079403 (data: 1.063282, reg: 0.016121)\n",
      "Iteration 1800, Loss: 1.079368 (data: 1.063273, reg: 0.016095)\n",
      "Iteration 1900, Loss: 1.079335 (data: 1.063266, reg: 0.016070)\n",
      "LR: 0.001, Iter: 2000, C: 0.1, Val Acc: 0.4161\n",
      "Iteration 0, Loss: 1.332007 (data: 1.099949, reg: 0.232058)\n",
      "Iteration 100, Loss: 1.257123 (data: 1.065780, reg: 0.191343)\n",
      "Iteration 200, Loss: 1.199741 (data: 1.039229, reg: 0.160512)\n",
      "Iteration 300, Loss: 1.155149 (data: 1.018027, reg: 0.137122)\n",
      "Iteration 400, Loss: 1.120136 (data: 1.000738, reg: 0.119398)\n",
      "Iteration 500, Loss: 1.092442 (data: 0.986424, reg: 0.106018)\n",
      "Iteration 600, Loss: 1.070422 (data: 0.974437, reg: 0.095985)\n",
      "Iteration 700, Loss: 1.052844 (data: 0.964314, reg: 0.088530)\n",
      "Iteration 800, Loss: 1.038772 (data: 0.955711, reg: 0.083062)\n",
      "Iteration 900, Loss: 1.027480 (data: 0.948359, reg: 0.079121)\n",
      "Iteration 1000, Loss: 1.018401 (data: 0.942052, reg: 0.076349)\n",
      "Iteration 1100, Loss: 1.011090 (data: 0.936621, reg: 0.074468)\n",
      "Iteration 1200, Loss: 1.005193 (data: 0.931931, reg: 0.073262)\n",
      "Iteration 1300, Loss: 1.000431 (data: 0.927870, reg: 0.072561)\n",
      "Iteration 1400, Loss: 0.996581 (data: 0.924345, reg: 0.072235)\n",
      "Iteration 1500, Loss: 0.993464 (data: 0.921280, reg: 0.072184)\n",
      "Iteration 1600, Loss: 0.990939 (data: 0.918610, reg: 0.072329)\n",
      "Iteration 1700, Loss: 0.988890 (data: 0.916280, reg: 0.072610)\n",
      "Iteration 1800, Loss: 0.987227 (data: 0.914244, reg: 0.072983)\n",
      "Iteration 1900, Loss: 0.985875 (data: 0.912462, reg: 0.073412)\n",
      "LR: 0.001, Iter: 2000, C: 1.0, Val Acc: 0.6242\n",
      "Iteration 0, Loss: 1.114914 (data: 1.092010, reg: 0.022904)\n",
      "Iteration 100, Loss: 1.079831 (data: 1.057159, reg: 0.022672)\n",
      "Iteration 200, Loss: 1.050640 (data: 1.027913, reg: 0.022727)\n",
      "Iteration 300, Loss: 1.025717 (data: 1.002713, reg: 0.023004)\n",
      "Iteration 400, Loss: 1.003988 (data: 0.980533, reg: 0.023455)\n",
      "Iteration 500, Loss: 0.984741 (data: 0.960693, reg: 0.024048)\n",
      "Iteration 600, Loss: 0.967493 (data: 0.942735, reg: 0.024758)\n",
      "Iteration 700, Loss: 0.951905 (data: 0.926337, reg: 0.025567)\n",
      "Iteration 800, Loss: 0.937725 (data: 0.911265, reg: 0.026459)\n",
      "Iteration 900, Loss: 0.924761 (data: 0.897339, reg: 0.027422)\n",
      "Iteration 1000, Loss: 0.912863 (data: 0.884419, reg: 0.028444)\n",
      "Iteration 1100, Loss: 0.901905 (data: 0.872388, reg: 0.029517)\n",
      "Iteration 1200, Loss: 0.891783 (data: 0.861152, reg: 0.030631)\n",
      "Iteration 1300, Loss: 0.882411 (data: 0.850629, reg: 0.031782)\n",
      "Iteration 1400, Loss: 0.873711 (data: 0.840750, reg: 0.032961)\n",
      "Iteration 1500, Loss: 0.865619 (data: 0.831454, reg: 0.034165)\n",
      "Iteration 1600, Loss: 0.858078 (data: 0.822690, reg: 0.035388)\n",
      "Iteration 1700, Loss: 0.851036 (data: 0.814410, reg: 0.036626)\n",
      "Iteration 1800, Loss: 0.844450 (data: 0.806575, reg: 0.037875)\n",
      "Iteration 1900, Loss: 0.838279 (data: 0.799147, reg: 0.039132)\n",
      "LR: 0.001, Iter: 2000, C: 10.0, Val Acc: 0.6577\n",
      "Iteration 0, Loss: 1.101416 (data: 1.099131, reg: 0.002285)\n",
      "Iteration 100, Loss: 1.064761 (data: 1.062463, reg: 0.002297)\n",
      "Iteration 200, Loss: 1.034041 (data: 1.031701, reg: 0.002340)\n",
      "Iteration 300, Loss: 1.007583 (data: 1.005177, reg: 0.002407)\n",
      "Iteration 400, Loss: 0.984274 (data: 0.981782, reg: 0.002492)\n",
      "Iteration 500, Loss: 0.963381 (data: 0.960787, reg: 0.002594)\n",
      "Iteration 600, Loss: 0.944412 (data: 0.941703, reg: 0.002709)\n",
      "Iteration 700, Loss: 0.927029 (data: 0.924193, reg: 0.002836)\n",
      "Iteration 800, Loss: 0.910986 (data: 0.908013, reg: 0.002973)\n",
      "Iteration 900, Loss: 0.896102 (data: 0.892982, reg: 0.003120)\n",
      "Iteration 1000, Loss: 0.882231 (data: 0.878958, reg: 0.003274)\n",
      "Iteration 1100, Loss: 0.869260 (data: 0.865825, reg: 0.003435)\n",
      "Iteration 1200, Loss: 0.857092 (data: 0.853489, reg: 0.003603)\n",
      "Iteration 1300, Loss: 0.845646 (data: 0.841869, reg: 0.003776)\n",
      "Iteration 1400, Loss: 0.834853 (data: 0.830898, reg: 0.003955)\n",
      "Iteration 1500, Loss: 0.824653 (data: 0.820514, reg: 0.004138)\n",
      "Iteration 1600, Loss: 0.814993 (data: 0.810667, reg: 0.004325)\n",
      "Iteration 1700, Loss: 0.805827 (data: 0.801311, reg: 0.004516)\n",
      "Iteration 1800, Loss: 0.797115 (data: 0.792404, reg: 0.004711)\n",
      "Iteration 1900, Loss: 0.788819 (data: 0.783911, reg: 0.004908)\n",
      "LR: 0.001, Iter: 2000, C: 100.0, Val Acc: 0.6242\n",
      "Iteration 0, Loss: 231.912388 (data: 1.101252, reg: 230.811136)\n",
      "Iteration 100, Loss: 596043094196816304914909668731554501366887245892671754465201238287810689508868963186653155547257231541600636323466491985920.000000 (data: 19.436994, reg: 596043094196816304914909668731554501366887245892671754465201238287810689508868963186653155547257231541600636323466491985920.000000)\n",
      "Iteration 200, Loss: 1539132207324272676045731778639114552927609128649792060436306126574427840826115170941403091653564213080120060737331442040611943695458937450637582637647254295735810444115801969843556174479656953183339905482024178872600158927221758751672285265920.000000 (data: 19.436994, reg: 1539132207324272676045731778639114552927609128649792060436306126574427840826115170941403091653564213080120060737331442040611943695458937450637582637647254295735810444115801969843556174479656953183339905482024178872600158927221758751672285265920.000000)\n",
      "Iteration 300, Loss: inf (data: 19.436994, reg: inf)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/jtg758yd6_n9rfwplmbsj_xh0000gn/T/ipykernel_52940/3521502296.py:52: RuntimeWarning: overflow encountered in multiply\n",
      "  reg_loss = (lambda_reg / 2) * np.sum(W * W)  # L2 regularization term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400, Loss: inf (data: 19.436994, reg: inf)\n",
      "LR: 0.005, Iter: 500, C: 0.001, Val Acc: 0.3221\n",
      "Iteration 0, Loss: 24.076361 (data: 1.103207, reg: 22.973153)\n",
      "Iteration 100, Loss: 1.095660 (data: 1.093772, reg: 0.001888)\n",
      "Iteration 200, Loss: 1.094944 (data: 1.093096, reg: 0.001848)\n",
      "Iteration 300, Loss: 1.094432 (data: 1.092616, reg: 0.001816)\n",
      "Iteration 400, Loss: 1.094065 (data: 1.092275, reg: 0.001790)\n",
      "LR: 0.005, Iter: 500, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.391403 (data: 1.106836, reg: 2.284567)\n",
      "Iteration 100, Loss: 1.080092 (data: 1.063650, reg: 0.016442)\n",
      "Iteration 200, Loss: 1.079705 (data: 1.063378, reg: 0.016327)\n",
      "Iteration 300, Loss: 1.079474 (data: 1.063301, reg: 0.016172)\n",
      "Iteration 400, Loss: 1.079300 (data: 1.063258, reg: 0.016042)\n",
      "LR: 0.005, Iter: 500, C: 0.1, Val Acc: 0.4161\n",
      "Iteration 0, Loss: 1.331686 (data: 1.100290, reg: 0.231396)\n",
      "Iteration 100, Loss: 1.091465 (data: 0.985410, reg: 0.106055)\n",
      "Iteration 200, Loss: 1.018177 (data: 0.941759, reg: 0.076418)\n",
      "Iteration 300, Loss: 0.993430 (data: 0.921273, reg: 0.072157)\n",
      "Iteration 400, Loss: 0.984784 (data: 0.910984, reg: 0.073800)\n",
      "LR: 0.005, Iter: 500, C: 1.0, Val Acc: 0.6309\n",
      "Iteration 0, Loss: 1.124561 (data: 1.101046, reg: 0.023515)\n",
      "Iteration 100, Loss: 0.991065 (data: 0.966773, reg: 0.024292)\n",
      "Iteration 200, Loss: 0.917359 (data: 0.888873, reg: 0.028486)\n",
      "Iteration 300, Loss: 0.869046 (data: 0.834965, reg: 0.034081)\n",
      "Iteration 400, Loss: 0.835223 (data: 0.794998, reg: 0.040225)\n",
      "LR: 0.005, Iter: 500, C: 10.0, Val Acc: 0.6711\n",
      "Iteration 0, Loss: 1.113092 (data: 1.110804, reg: 0.002288)\n",
      "Iteration 100, Loss: 0.967660 (data: 0.965092, reg: 0.002567)\n",
      "Iteration 200, Loss: 0.884282 (data: 0.881040, reg: 0.003242)\n",
      "Iteration 300, Loss: 0.825710 (data: 0.821602, reg: 0.004108)\n",
      "Iteration 400, Loss: 0.781466 (data: 0.776385, reg: 0.005081)\n",
      "LR: 0.005, Iter: 500, C: 100.0, Val Acc: 0.6913\n",
      "Iteration 0, Loss: 221.539753 (data: 1.104868, reg: 220.434885)\n",
      "Iteration 100, Loss: 569258295219375929477811184892354886825124296318659093565185835445829510486807470611997352377765315925886258107428150181888.000000 (data: 19.246435, reg: 569258295219375929477811184892354886825124296318659093565185835445829510486807470611997352377765315925886258107428150181888.000000)\n",
      "Iteration 200, Loss: 1469967163430194912717663373042938078990453326579988433250161309585509716201863668831323843924757381794800895938389047545452020259853422719002192608357499088662791252058345798185569406056957142622054040735396344411339289162096782300030475173888.000000 (data: 19.246435, reg: 1469967163430194912717663373042938078990453326579988433250161309585509716201863668831323843924757381794800895938389047545452020259853422719002192608357499088662791252058345798185569406056957142622054040735396344411339289162096782300030475173888.000000)\n",
      "Iteration 300, Loss: inf (data: 19.246435, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 19.246435, reg: inf)\n",
      "Iteration 500, Loss: inf (data: 19.246435, reg: inf)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.005, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.152216 (data: 1.105328, reg: 22.046888)\n",
      "Iteration 100, Loss: 1.095658 (data: 1.093770, reg: 0.001888)\n",
      "Iteration 200, Loss: 1.094943 (data: 1.093095, reg: 0.001848)\n",
      "Iteration 300, Loss: 1.094431 (data: 1.092615, reg: 0.001816)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/jtg758yd6_n9rfwplmbsj_xh0000gn/T/ipykernel_52940/3521502296.py:57: RuntimeWarning: overflow encountered in multiply\n",
      "  dW = (X.T @ (y_pred - y_one_hot)) / n_samples + lambda_reg * W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400, Loss: 1.094064 (data: 1.092274, reg: 0.001790)\n",
      "Iteration 500, Loss: 1.093802 (data: 1.092033, reg: 0.001769)\n",
      "Iteration 600, Loss: 1.093615 (data: 1.091863, reg: 0.001752)\n",
      "Iteration 700, Loss: 1.093482 (data: 1.091744, reg: 0.001738)\n",
      "Iteration 800, Loss: 1.093387 (data: 1.091661, reg: 0.001726)\n",
      "Iteration 900, Loss: 1.093319 (data: 1.091603, reg: 0.001717)\n",
      "LR: 0.005, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.276099 (data: 1.097766, reg: 2.178333)\n",
      "Iteration 100, Loss: 1.080059 (data: 1.063588, reg: 0.016471)\n",
      "Iteration 200, Loss: 1.079683 (data: 1.063370, reg: 0.016314)\n",
      "Iteration 300, Loss: 1.079457 (data: 1.063297, reg: 0.016161)\n",
      "Iteration 400, Loss: 1.079287 (data: 1.063255, reg: 0.016032)\n",
      "Iteration 500, Loss: 1.079159 (data: 1.063236, reg: 0.015924)\n",
      "Iteration 600, Loss: 1.079063 (data: 1.063231, reg: 0.015832)\n",
      "Iteration 700, Loss: 1.078991 (data: 1.063236, reg: 0.015755)\n",
      "Iteration 800, Loss: 1.078936 (data: 1.063247, reg: 0.015689)\n",
      "Iteration 900, Loss: 1.078895 (data: 1.063262, reg: 0.015633)\n",
      "LR: 0.005, Iter: 1000, C: 0.1, Val Acc: 0.3826\n",
      "Iteration 0, Loss: 1.328102 (data: 1.094765, reg: 0.233337)\n",
      "Iteration 100, Loss: 1.090956 (data: 0.983389, reg: 0.107567)\n",
      "Iteration 200, Loss: 1.017960 (data: 0.940532, reg: 0.077429)\n",
      "Iteration 300, Loss: 0.993334 (data: 0.920512, reg: 0.072822)\n",
      "Iteration 400, Loss: 0.984739 (data: 0.910513, reg: 0.074226)\n",
      "Iteration 500, Loss: 0.981658 (data: 0.905326, reg: 0.076332)\n",
      "Iteration 600, Loss: 0.980515 (data: 0.902552, reg: 0.077963)\n",
      "Iteration 700, Loss: 0.980065 (data: 0.901020, reg: 0.079045)\n",
      "Iteration 800, Loss: 0.979868 (data: 0.900139, reg: 0.079729)\n",
      "Iteration 900, Loss: 0.979766 (data: 0.899606, reg: 0.080160)\n",
      "LR: 0.005, Iter: 1000, C: 1.0, Val Acc: 0.6510\n",
      "Iteration 0, Loss: 1.123798 (data: 1.101291, reg: 0.022507)\n",
      "Iteration 100, Loss: 0.988478 (data: 0.965016, reg: 0.023462)\n",
      "Iteration 200, Loss: 0.915133 (data: 0.887309, reg: 0.027824)\n",
      "Iteration 300, Loss: 0.867162 (data: 0.833608, reg: 0.033555)\n",
      "Iteration 400, Loss: 0.833616 (data: 0.793805, reg: 0.039812)\n",
      "Iteration 500, Loss: 0.809127 (data: 0.762965, reg: 0.046163)\n",
      "Iteration 600, Loss: 0.790657 (data: 0.738277, reg: 0.052380)\n",
      "Iteration 700, Loss: 0.776361 (data: 0.718013, reg: 0.058348)\n",
      "Iteration 800, Loss: 0.765061 (data: 0.701052, reg: 0.064009)\n",
      "Iteration 900, Loss: 0.755974 (data: 0.686631, reg: 0.069343)\n",
      "LR: 0.005, Iter: 1000, C: 10.0, Val Acc: 0.7047\n",
      "Iteration 0, Loss: 1.106839 (data: 1.104517, reg: 0.002323)\n",
      "Iteration 100, Loss: 0.969990 (data: 0.967392, reg: 0.002598)\n",
      "Iteration 200, Loss: 0.887780 (data: 0.884530, reg: 0.003250)\n",
      "Iteration 300, Loss: 0.829345 (data: 0.825250, reg: 0.004095)\n",
      "Iteration 400, Loss: 0.784984 (data: 0.779936, reg: 0.005049)\n",
      "Iteration 500, Loss: 0.749809 (data: 0.743743, reg: 0.006066)\n",
      "Iteration 600, Loss: 0.720984 (data: 0.713863, reg: 0.007121)\n",
      "Iteration 700, Loss: 0.696746 (data: 0.688549, reg: 0.008196)\n",
      "Iteration 800, Loss: 0.675939 (data: 0.666657, reg: 0.009282)\n",
      "Iteration 900, Loss: 0.657777 (data: 0.647403, reg: 0.010373)\n",
      "LR: 0.005, Iter: 1000, C: 100.0, Val Acc: 0.7315\n",
      "Iteration 0, Loss: 228.080886 (data: 1.098535, reg: 226.982351)\n",
      "Iteration 100, Loss: 586146738527090002358517829008046371287009070034912547963801137402593669265426706126212674547722240325555225216988838100992.000000 (data: 18.738279, reg: 586146738527090002358517829008046371287009070034912547963801137402593669265426706126212674547722240325555225216988838100992.000000)\n",
      "Iteration 200, Loss: 1513577344102617244662108590004792834156342031221967150985600950304649808734444440084828299584814431330862258940778211646090112680442298431018522461220804992718150550756552303720834962323621516389288730321993721673566953465068998942651939028992.000000 (data: 18.738279, reg: 1513577344102617244662108590004792834156342031221967150985600950304649808734444440084828299584814431330862258940778211646090112680442298431018522461220804992718150550756552303720834962323621516389288730321993721673566953465068998942651939028992.000000)\n",
      "Iteration 300, Loss: inf (data: 18.738279, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 18.738279, reg: inf)\n",
      "Iteration 500, Loss: inf (data: 18.738279, reg: inf)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.005, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 22.715176 (data: 1.108549, reg: 21.606627)\n",
      "Iteration 100, Loss: 1.095659 (data: 1.093771, reg: 0.001888)\n",
      "Iteration 200, Loss: 1.094944 (data: 1.093096, reg: 0.001848)\n",
      "Iteration 300, Loss: 1.094431 (data: 1.092616, reg: 0.001816)\n",
      "Iteration 400, Loss: 1.094064 (data: 1.092275, reg: 0.001790)\n",
      "Iteration 500, Loss: 1.093802 (data: 1.092034, reg: 0.001769)\n",
      "Iteration 600, Loss: 1.093615 (data: 1.091864, reg: 0.001752)\n",
      "Iteration 700, Loss: 1.093482 (data: 1.091744, reg: 0.001738)\n",
      "Iteration 800, Loss: 1.093387 (data: 1.091661, reg: 0.001726)\n",
      "Iteration 900, Loss: 1.093319 (data: 1.091603, reg: 0.001717)\n",
      "Iteration 1000, Loss: 1.093271 (data: 1.091562, reg: 0.001709)\n",
      "Iteration 1100, Loss: 1.093237 (data: 1.091535, reg: 0.001703)\n",
      "Iteration 1200, Loss: 1.093213 (data: 1.091516, reg: 0.001697)\n",
      "Iteration 1300, Loss: 1.093196 (data: 1.091503, reg: 0.001693)\n",
      "Iteration 1400, Loss: 1.093184 (data: 1.091494, reg: 0.001689)\n",
      "Iteration 1500, Loss: 1.093175 (data: 1.091489, reg: 0.001686)\n",
      "Iteration 1600, Loss: 1.093169 (data: 1.091485, reg: 0.001684)\n",
      "Iteration 1700, Loss: 1.093164 (data: 1.091483, reg: 0.001681)\n",
      "Iteration 1800, Loss: 1.093161 (data: 1.091482, reg: 0.001680)\n",
      "Iteration 1900, Loss: 1.093159 (data: 1.091481, reg: 0.001678)\n",
      "LR: 0.005, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.371558 (data: 1.096268, reg: 2.275290)\n",
      "Iteration 100, Loss: 1.080121 (data: 1.063640, reg: 0.016481)\n",
      "Iteration 200, Loss: 1.079727 (data: 1.063386, reg: 0.016341)\n",
      "Iteration 300, Loss: 1.079490 (data: 1.063306, reg: 0.016184)\n",
      "Iteration 400, Loss: 1.079312 (data: 1.063261, reg: 0.016052)\n",
      "Iteration 500, Loss: 1.079178 (data: 1.063238, reg: 0.015940)\n",
      "Iteration 600, Loss: 1.079077 (data: 1.063231, reg: 0.015846)\n",
      "Iteration 700, Loss: 1.079001 (data: 1.063235, reg: 0.015766)\n",
      "Iteration 800, Loss: 1.078944 (data: 1.063245, reg: 0.015699)\n",
      "Iteration 900, Loss: 1.078901 (data: 1.063260, reg: 0.015642)\n",
      "Iteration 1000, Loss: 1.078869 (data: 1.063276, reg: 0.015593)\n",
      "Iteration 1100, Loss: 1.078845 (data: 1.063294, reg: 0.015551)\n",
      "Iteration 1200, Loss: 1.078827 (data: 1.063312, reg: 0.015515)\n",
      "Iteration 1300, Loss: 1.078813 (data: 1.063328, reg: 0.015485)\n",
      "Iteration 1400, Loss: 1.078803 (data: 1.063344, reg: 0.015458)\n",
      "Iteration 1500, Loss: 1.078795 (data: 1.063359, reg: 0.015436)\n",
      "Iteration 1600, Loss: 1.078789 (data: 1.063373, reg: 0.015417)\n",
      "Iteration 1700, Loss: 1.078785 (data: 1.063385, reg: 0.015400)\n",
      "Iteration 1800, Loss: 1.078782 (data: 1.063396, reg: 0.015386)\n",
      "Iteration 1900, Loss: 1.078779 (data: 1.063406, reg: 0.015373)\n",
      "LR: 0.005, Iter: 2000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.350962 (data: 1.114686, reg: 0.236275)\n",
      "Iteration 100, Loss: 1.095660 (data: 0.989840, reg: 0.105820)\n",
      "Iteration 200, Loss: 1.019210 (data: 0.943342, reg: 0.075867)\n",
      "Iteration 300, Loss: 0.993700 (data: 0.921855, reg: 0.071845)\n",
      "Iteration 400, Loss: 0.984856 (data: 0.911185, reg: 0.073671)\n",
      "Iteration 500, Loss: 0.981701 (data: 0.905674, reg: 0.076027)\n",
      "Iteration 600, Loss: 0.980534 (data: 0.902738, reg: 0.077796)\n",
      "Iteration 700, Loss: 0.980076 (data: 0.901124, reg: 0.078952)\n",
      "Iteration 800, Loss: 0.979876 (data: 0.900201, reg: 0.079675)\n",
      "Iteration 900, Loss: 0.979772 (data: 0.899646, reg: 0.080127)\n",
      "Iteration 1000, Loss: 0.979707 (data: 0.899290, reg: 0.080417)\n",
      "Iteration 1100, Loss: 0.979658 (data: 0.899044, reg: 0.080615)\n",
      "Iteration 1200, Loss: 0.979619 (data: 0.898861, reg: 0.080758)\n",
      "Iteration 1300, Loss: 0.979585 (data: 0.898715, reg: 0.080869)\n",
      "Iteration 1400, Loss: 0.979555 (data: 0.898593, reg: 0.080962)\n",
      "Iteration 1500, Loss: 0.979529 (data: 0.898487, reg: 0.081042)\n",
      "Iteration 1600, Loss: 0.979506 (data: 0.898391, reg: 0.081114)\n",
      "Iteration 1700, Loss: 0.979485 (data: 0.898304, reg: 0.081181)\n",
      "Iteration 1800, Loss: 0.979466 (data: 0.898224, reg: 0.081243)\n",
      "Iteration 1900, Loss: 0.979450 (data: 0.898149, reg: 0.081301)\n",
      "LR: 0.005, Iter: 2000, C: 1.0, Val Acc: 0.6577\n",
      "Iteration 0, Loss: 1.127109 (data: 1.104239, reg: 0.022870)\n",
      "Iteration 100, Loss: 0.990056 (data: 0.966354, reg: 0.023702)\n",
      "Iteration 200, Loss: 0.916342 (data: 0.888352, reg: 0.027991)\n",
      "Iteration 300, Loss: 0.868247 (data: 0.834593, reg: 0.033654)\n",
      "Iteration 400, Loss: 0.834617 (data: 0.794769, reg: 0.039848)\n",
      "Iteration 500, Loss: 0.810045 (data: 0.763903, reg: 0.046142)\n",
      "Iteration 600, Loss: 0.791488 (data: 0.739178, reg: 0.052311)\n",
      "Iteration 700, Loss: 0.777106 (data: 0.718868, reg: 0.058237)\n",
      "Iteration 800, Loss: 0.765722 (data: 0.701857, reg: 0.063866)\n",
      "Iteration 900, Loss: 0.756557 (data: 0.687384, reg: 0.069173)\n",
      "Iteration 1000, Loss: 0.749071 (data: 0.674915, reg: 0.074156)\n",
      "Iteration 1100, Loss: 0.742883 (data: 0.664062, reg: 0.078822)\n",
      "Iteration 1200, Loss: 0.737716 (data: 0.654534, reg: 0.083182)\n",
      "Iteration 1300, Loss: 0.733364 (data: 0.646110, reg: 0.087254)\n",
      "Iteration 1400, Loss: 0.729670 (data: 0.638616, reg: 0.091054)\n",
      "Iteration 1500, Loss: 0.726514 (data: 0.631915, reg: 0.094598)\n",
      "Iteration 1600, Loss: 0.723802 (data: 0.625897, reg: 0.097905)\n",
      "Iteration 1700, Loss: 0.721461 (data: 0.620470, reg: 0.100990)\n",
      "Iteration 1800, Loss: 0.719429 (data: 0.615560, reg: 0.103869)\n",
      "Iteration 1900, Loss: 0.717659 (data: 0.611104, reg: 0.106555)\n",
      "LR: 0.005, Iter: 2000, C: 10.0, Val Acc: 0.7383\n",
      "Iteration 0, Loss: 1.115145 (data: 1.112938, reg: 0.002206)\n",
      "Iteration 100, Loss: 0.968878 (data: 0.966397, reg: 0.002481)\n",
      "Iteration 200, Loss: 0.886209 (data: 0.883062, reg: 0.003147)\n",
      "Iteration 300, Loss: 0.827923 (data: 0.823922, reg: 0.004002)\n",
      "Iteration 400, Loss: 0.783737 (data: 0.778774, reg: 0.004963)\n",
      "Iteration 500, Loss: 0.748714 (data: 0.742727, reg: 0.005987)\n",
      "Iteration 600, Loss: 0.720019 (data: 0.712973, reg: 0.007046)\n",
      "Iteration 700, Loss: 0.695891 (data: 0.687766, reg: 0.008125)\n",
      "Iteration 800, Loss: 0.675182 (data: 0.665967, reg: 0.009215)\n",
      "Iteration 900, Loss: 0.657105 (data: 0.646796, reg: 0.010309)\n",
      "Iteration 1000, Loss: 0.641108 (data: 0.629705, reg: 0.011403)\n",
      "Iteration 1100, Loss: 0.626789 (data: 0.614294, reg: 0.012495)\n",
      "Iteration 1200, Loss: 0.613849 (data: 0.600265, reg: 0.013584)\n",
      "Iteration 1300, Loss: 0.602061 (data: 0.587393, reg: 0.014668)\n",
      "Iteration 1400, Loss: 0.591249 (data: 0.575502, reg: 0.015747)\n",
      "Iteration 1500, Loss: 0.581274 (data: 0.564454, reg: 0.016820)\n",
      "Iteration 1600, Loss: 0.572025 (data: 0.554138, reg: 0.017887)\n",
      "Iteration 1700, Loss: 0.563412 (data: 0.544464, reg: 0.018948)\n",
      "Iteration 1800, Loss: 0.555362 (data: 0.535359, reg: 0.020004)\n",
      "Iteration 1900, Loss: 0.547813 (data: 0.526761, reg: 0.021053)\n",
      "LR: 0.005, Iter: 2000, C: 100.0, Val Acc: 0.7315\n",
      "Iteration 0, Loss: 234.277370 (data: 1.100179, reg: 233.177191)\n",
      "Iteration 100, Loss: 16451281864236123776285260387185533254079492918883194840211083799795023804185655248760090809364394896104900577224181752140799573240437567211702075971413974355966832137569453849991732068834344960.000000 (data: 18.992357, reg: 16451281864236123776285260387185533254079492918883194840211083799795023804185655248760090809364394896104900577224181752140799573240437567211702075971413974355966832137569453849991732068834344960.000000)\n",
      "Iteration 200, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.01, Iter: 500, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.886543 (data: 1.110015, reg: 22.776528)\n",
      "Iteration 100, Loss: 1.094943 (data: 1.093095, reg: 0.001848)\n",
      "Iteration 200, Loss: 1.094063 (data: 1.092274, reg: 0.001790)\n",
      "Iteration 300, Loss: 1.093614 (data: 1.091863, reg: 0.001751)\n",
      "Iteration 400, Loss: 1.093386 (data: 1.091660, reg: 0.001726)\n",
      "LR: 0.01, Iter: 500, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.426077 (data: 1.104383, reg: 2.321694)\n",
      "Iteration 100, Loss: 1.079714 (data: 1.063381, reg: 0.016333)\n",
      "Iteration 200, Loss: 1.079305 (data: 1.063259, reg: 0.016046)\n",
      "Iteration 300, Loss: 1.079073 (data: 1.063231, reg: 0.015842)\n",
      "Iteration 400, Loss: 1.078942 (data: 1.063246, reg: 0.015696)\n",
      "LR: 0.01, Iter: 500, C: 0.1, Val Acc: 0.3826\n",
      "Iteration 0, Loss: 1.333106 (data: 1.101391, reg: 0.231715)\n",
      "Iteration 100, Loss: 1.018135 (data: 0.942219, reg: 0.075916)\n",
      "Iteration 200, Loss: 0.984680 (data: 0.910877, reg: 0.073803)\n",
      "Iteration 300, Loss: 0.980475 (data: 0.902596, reg: 0.077880)\n",
      "Iteration 400, Loss: 0.979841 (data: 0.900105, reg: 0.079736)\n",
      "LR: 0.01, Iter: 500, C: 1.0, Val Acc: 0.6510\n",
      "Iteration 0, Loss: 1.123981 (data: 1.100773, reg: 0.023208)\n",
      "Iteration 100, Loss: 0.913816 (data: 0.885309, reg: 0.028507)\n",
      "Iteration 200, Loss: 0.832372 (data: 0.791823, reg: 0.040549)\n",
      "Iteration 300, Loss: 0.789797 (data: 0.736706, reg: 0.053090)\n",
      "Iteration 400, Loss: 0.764487 (data: 0.699841, reg: 0.064646)\n",
      "LR: 0.01, Iter: 500, C: 10.0, Val Acc: 0.7181\n",
      "Iteration 0, Loss: 1.105707 (data: 1.103521, reg: 0.002186)\n",
      "Iteration 100, Loss: 0.885231 (data: 0.882087, reg: 0.003143)\n",
      "Iteration 200, Loss: 0.783041 (data: 0.778079, reg: 0.004962)\n",
      "Iteration 300, Loss: 0.719444 (data: 0.712396, reg: 0.007048)\n",
      "Iteration 400, Loss: 0.674671 (data: 0.665451, reg: 0.009219)\n",
      "LR: 0.01, Iter: 500, C: 100.0, Val Acc: 0.7248\n",
      "Iteration 0, Loss: 225.685533 (data: 1.100259, reg: 224.585274)\n",
      "Iteration 100, Loss: 15845178497303519332030869835843314237339855148517558070529141978341061789378825680587344849267230702478063497990348274418110939115559552877279644520212690074326630940710422598657856103906279424.000000 (data: 18.420681, reg: 15845178497303519332030869835843314237339855148517558070529141978341061789378825680587344849267230702478063497990348274418110939115559552877279644520212690074326630940710422598657856103906279424.000000)\n",
      "Iteration 200, Loss: inf (data: 18.420681, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 18.420681, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.01, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.492142 (data: 1.093546, reg: 22.398596)\n",
      "Iteration 100, Loss: 1.094944 (data: 1.093096, reg: 0.001848)\n",
      "Iteration 200, Loss: 1.094064 (data: 1.092274, reg: 0.001790)\n",
      "Iteration 300, Loss: 1.093615 (data: 1.091863, reg: 0.001751)\n",
      "Iteration 400, Loss: 1.093386 (data: 1.091660, reg: 0.001726)\n",
      "Iteration 500, Loss: 1.093271 (data: 1.091562, reg: 0.001709)\n",
      "Iteration 600, Loss: 1.093213 (data: 1.091516, reg: 0.001697)\n",
      "Iteration 700, Loss: 1.093183 (data: 1.091494, reg: 0.001689)\n",
      "Iteration 800, Loss: 1.093169 (data: 1.091485, reg: 0.001684)\n",
      "Iteration 900, Loss: 1.093161 (data: 1.091482, reg: 0.001680)\n",
      "LR: 0.01, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.394398 (data: 1.092712, reg: 2.301686)\n",
      "Iteration 100, Loss: 1.079725 (data: 1.063385, reg: 0.016340)\n",
      "Iteration 200, Loss: 1.079311 (data: 1.063260, reg: 0.016050)\n",
      "Iteration 300, Loss: 1.079076 (data: 1.063231, reg: 0.015845)\n",
      "Iteration 400, Loss: 1.078944 (data: 1.063245, reg: 0.015698)\n",
      "Iteration 500, Loss: 1.078869 (data: 1.063277, reg: 0.015592)\n",
      "Iteration 600, Loss: 1.078826 (data: 1.063312, reg: 0.015515)\n",
      "Iteration 700, Loss: 1.078803 (data: 1.063345, reg: 0.015458)\n",
      "Iteration 800, Loss: 1.078789 (data: 1.063373, reg: 0.015416)\n",
      "Iteration 900, Loss: 1.078782 (data: 1.063396, reg: 0.015385)\n",
      "LR: 0.01, Iter: 1000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.329193 (data: 1.100825, reg: 0.228368)\n",
      "Iteration 100, Loss: 1.017506 (data: 0.941540, reg: 0.075966)\n",
      "Iteration 200, Loss: 0.984661 (data: 0.910841, reg: 0.073820)\n",
      "Iteration 300, Loss: 0.980507 (data: 0.902674, reg: 0.077832)\n",
      "Iteration 400, Loss: 0.979871 (data: 0.900191, reg: 0.079680)\n",
      "Iteration 500, Loss: 0.979705 (data: 0.899287, reg: 0.080418)\n",
      "Iteration 600, Loss: 0.979618 (data: 0.898859, reg: 0.080759)\n",
      "Iteration 700, Loss: 0.979554 (data: 0.898591, reg: 0.080963)\n",
      "Iteration 800, Loss: 0.979505 (data: 0.898388, reg: 0.081116)\n",
      "Iteration 900, Loss: 0.979466 (data: 0.898221, reg: 0.081245)\n",
      "LR: 0.01, Iter: 1000, C: 1.0, Val Acc: 0.6577\n",
      "Iteration 0, Loss: 1.114508 (data: 1.092121, reg: 0.022387)\n",
      "Iteration 100, Loss: 0.912026 (data: 0.883948, reg: 0.028078)\n",
      "Iteration 200, Loss: 0.831771 (data: 0.791605, reg: 0.040166)\n",
      "Iteration 300, Loss: 0.789412 (data: 0.736657, reg: 0.052755)\n",
      "Iteration 400, Loss: 0.764145 (data: 0.699771, reg: 0.064375)\n",
      "Iteration 500, Loss: 0.747847 (data: 0.673155, reg: 0.074692)\n",
      "Iteration 600, Loss: 0.736751 (data: 0.653030, reg: 0.083721)\n",
      "Iteration 700, Loss: 0.728900 (data: 0.637320, reg: 0.091580)\n",
      "Iteration 800, Loss: 0.723182 (data: 0.624773, reg: 0.098409)\n",
      "Iteration 900, Loss: 0.718925 (data: 0.614579, reg: 0.104345)\n",
      "LR: 0.01, Iter: 1000, C: 10.0, Val Acc: 0.7383\n",
      "Iteration 0, Loss: 1.109691 (data: 1.107341, reg: 0.002350)\n",
      "Iteration 100, Loss: 0.887788 (data: 0.884519, reg: 0.003270)\n",
      "Iteration 200, Loss: 0.784563 (data: 0.779488, reg: 0.005076)\n",
      "Iteration 300, Loss: 0.720594 (data: 0.713442, reg: 0.007152)\n",
      "Iteration 400, Loss: 0.675616 (data: 0.666300, reg: 0.009316)\n",
      "Iteration 500, Loss: 0.641427 (data: 0.629927, reg: 0.011500)\n",
      "Iteration 600, Loss: 0.614063 (data: 0.600385, reg: 0.013678)\n",
      "Iteration 700, Loss: 0.591370 (data: 0.575530, reg: 0.015840)\n",
      "Iteration 800, Loss: 0.572065 (data: 0.554085, reg: 0.017980)\n",
      "Iteration 900, Loss: 0.555334 (data: 0.535237, reg: 0.020098)\n",
      "LR: 0.01, Iter: 1000, C: 100.0, Val Acc: 0.7315\n",
      "Iteration 0, Loss: 228.860064 (data: 1.100625, reg: 227.759439)\n",
      "Iteration 100, Loss: 16069281239300456925708434772242964601666104846962523187930027859121443138961690563972363335593362729983527255318023962311570997746428996518673554079810717844355679959860514435350461161786048512.000000 (data: 17.976044, reg: 16069281239300456925708434772242964601666104846962523187930027859121443138961690563972363335593362729983527255318023962311570997746428996518673554079810717844355679959860514435350461161786048512.000000)\n",
      "Iteration 200, Loss: inf (data: 17.976044, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 17.976044, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.01, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.856108 (data: 1.097770, reg: 22.758337)\n",
      "Iteration 100, Loss: 1.094946 (data: 1.093098, reg: 0.001848)\n",
      "Iteration 200, Loss: 1.094065 (data: 1.092275, reg: 0.001790)\n",
      "Iteration 300, Loss: 1.093615 (data: 1.091864, reg: 0.001752)\n",
      "Iteration 400, Loss: 1.093387 (data: 1.091661, reg: 0.001726)\n",
      "Iteration 500, Loss: 1.093271 (data: 1.091562, reg: 0.001709)\n",
      "Iteration 600, Loss: 1.093213 (data: 1.091516, reg: 0.001697)\n",
      "Iteration 700, Loss: 1.093183 (data: 1.091494, reg: 0.001689)\n",
      "Iteration 800, Loss: 1.093169 (data: 1.091485, reg: 0.001684)\n",
      "Iteration 900, Loss: 1.093161 (data: 1.091482, reg: 0.001680)\n",
      "Iteration 1000, Loss: 1.093157 (data: 1.091481, reg: 0.001677)\n",
      "Iteration 1100, Loss: 1.093156 (data: 1.091481, reg: 0.001675)\n",
      "Iteration 1200, Loss: 1.093155 (data: 1.091481, reg: 0.001674)\n",
      "Iteration 1300, Loss: 1.093154 (data: 1.091482, reg: 0.001673)\n",
      "Iteration 1400, Loss: 1.093154 (data: 1.091482, reg: 0.001672)\n",
      "Iteration 1500, Loss: 1.093154 (data: 1.091482, reg: 0.001671)\n",
      "Iteration 1600, Loss: 1.093154 (data: 1.091483, reg: 0.001671)\n",
      "Iteration 1700, Loss: 1.093154 (data: 1.091483, reg: 0.001671)\n",
      "Iteration 1800, Loss: 1.093154 (data: 1.091483, reg: 0.001671)\n",
      "Iteration 1900, Loss: 1.093154 (data: 1.091483, reg: 0.001671)\n",
      "LR: 0.01, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.337533 (data: 1.098835, reg: 2.238698)\n",
      "Iteration 100, Loss: 1.079724 (data: 1.063385, reg: 0.016340)\n",
      "Iteration 200, Loss: 1.079310 (data: 1.063260, reg: 0.016050)\n",
      "Iteration 300, Loss: 1.079076 (data: 1.063231, reg: 0.015845)\n",
      "Iteration 400, Loss: 1.078943 (data: 1.063245, reg: 0.015698)\n",
      "Iteration 500, Loss: 1.078869 (data: 1.063277, reg: 0.015592)\n",
      "Iteration 600, Loss: 1.078826 (data: 1.063312, reg: 0.015515)\n",
      "Iteration 700, Loss: 1.078803 (data: 1.063345, reg: 0.015458)\n",
      "Iteration 800, Loss: 1.078789 (data: 1.063373, reg: 0.015416)\n",
      "Iteration 900, Loss: 1.078782 (data: 1.063396, reg: 0.015385)\n",
      "Iteration 1000, Loss: 1.078778 (data: 1.063415, reg: 0.015362)\n",
      "Iteration 1100, Loss: 1.078775 (data: 1.063430, reg: 0.015345)\n",
      "Iteration 1200, Loss: 1.078774 (data: 1.063441, reg: 0.015333)\n",
      "Iteration 1300, Loss: 1.078773 (data: 1.063450, reg: 0.015323)\n",
      "Iteration 1400, Loss: 1.078773 (data: 1.063457, reg: 0.015316)\n",
      "Iteration 1500, Loss: 1.078772 (data: 1.063462, reg: 0.015311)\n",
      "Iteration 1600, Loss: 1.078772 (data: 1.063465, reg: 0.015307)\n",
      "Iteration 1700, Loss: 1.078772 (data: 1.063468, reg: 0.015304)\n",
      "Iteration 1800, Loss: 1.078772 (data: 1.063471, reg: 0.015302)\n",
      "Iteration 1900, Loss: 1.078772 (data: 1.063472, reg: 0.015300)\n",
      "LR: 0.01, Iter: 2000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.324888 (data: 1.093521, reg: 0.231367)\n",
      "Iteration 100, Loss: 1.017413 (data: 0.940009, reg: 0.077404)\n",
      "Iteration 200, Loss: 0.984679 (data: 0.910422, reg: 0.074257)\n",
      "Iteration 300, Loss: 0.980521 (data: 0.902559, reg: 0.077963)\n",
      "Iteration 400, Loss: 0.979882 (data: 0.900168, reg: 0.079713)\n",
      "Iteration 500, Loss: 0.979714 (data: 0.899294, reg: 0.080419)\n",
      "Iteration 600, Loss: 0.979624 (data: 0.898874, reg: 0.080750)\n",
      "Iteration 700, Loss: 0.979560 (data: 0.898608, reg: 0.080952)\n",
      "Iteration 800, Loss: 0.979509 (data: 0.898405, reg: 0.081104)\n",
      "Iteration 900, Loss: 0.979469 (data: 0.898236, reg: 0.081233)\n",
      "Iteration 1000, Loss: 0.979437 (data: 0.898090, reg: 0.081347)\n",
      "Iteration 1100, Loss: 0.979412 (data: 0.897962, reg: 0.081450)\n",
      "Iteration 1200, Loss: 0.979392 (data: 0.897850, reg: 0.081542)\n",
      "Iteration 1300, Loss: 0.979377 (data: 0.897751, reg: 0.081625)\n",
      "Iteration 1400, Loss: 0.979364 (data: 0.897664, reg: 0.081700)\n",
      "Iteration 1500, Loss: 0.979354 (data: 0.897587, reg: 0.081767)\n",
      "Iteration 1600, Loss: 0.979346 (data: 0.897518, reg: 0.081828)\n",
      "Iteration 1700, Loss: 0.979340 (data: 0.897458, reg: 0.081882)\n",
      "Iteration 1800, Loss: 0.979335 (data: 0.897404, reg: 0.081931)\n",
      "Iteration 1900, Loss: 0.979331 (data: 0.897356, reg: 0.081975)\n",
      "LR: 0.01, Iter: 2000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.133177 (data: 1.110116, reg: 0.023062)\n",
      "Iteration 100, Loss: 0.919051 (data: 0.891188, reg: 0.027863)\n",
      "Iteration 200, Loss: 0.836277 (data: 0.796682, reg: 0.039595)\n",
      "Iteration 300, Loss: 0.792583 (data: 0.740577, reg: 0.052007)\n",
      "Iteration 400, Loss: 0.766477 (data: 0.702931, reg: 0.063546)\n",
      "Iteration 500, Loss: 0.749603 (data: 0.675763, reg: 0.073839)\n",
      "Iteration 600, Loss: 0.738094 (data: 0.655214, reg: 0.082880)\n",
      "Iteration 700, Loss: 0.729939 (data: 0.639167, reg: 0.090773)\n",
      "Iteration 800, Loss: 0.723995 (data: 0.626345, reg: 0.097649)\n",
      "Iteration 900, Loss: 0.719566 (data: 0.615926, reg: 0.103640)\n",
      "Iteration 1000, Loss: 0.716209 (data: 0.607349, reg: 0.108860)\n",
      "Iteration 1100, Loss: 0.713627 (data: 0.600213, reg: 0.113414)\n",
      "Iteration 1200, Loss: 0.711616 (data: 0.594226, reg: 0.117391)\n",
      "Iteration 1300, Loss: 0.710032 (data: 0.589166, reg: 0.120866)\n",
      "Iteration 1400, Loss: 0.708770 (data: 0.584865, reg: 0.123906)\n",
      "Iteration 1500, Loss: 0.707756 (data: 0.581188, reg: 0.126567)\n",
      "Iteration 1600, Loss: 0.706931 (data: 0.578031, reg: 0.128900)\n",
      "Iteration 1700, Loss: 0.706255 (data: 0.575308, reg: 0.130946)\n",
      "Iteration 1800, Loss: 0.705694 (data: 0.572951, reg: 0.132743)\n",
      "Iteration 1900, Loss: 0.705224 (data: 0.570903, reg: 0.134321)\n",
      "LR: 0.01, Iter: 2000, C: 10.0, Val Acc: 0.7450\n",
      "Iteration 0, Loss: 1.110530 (data: 1.108256, reg: 0.002274)\n",
      "Iteration 100, Loss: 0.885598 (data: 0.882371, reg: 0.003228)\n",
      "Iteration 200, Loss: 0.783712 (data: 0.778672, reg: 0.005040)\n",
      "Iteration 300, Loss: 0.720179 (data: 0.713063, reg: 0.007116)\n",
      "Iteration 400, Loss: 0.675363 (data: 0.666084, reg: 0.009279)\n",
      "Iteration 500, Loss: 0.641237 (data: 0.629774, reg: 0.011462)\n",
      "Iteration 600, Loss: 0.613897 (data: 0.600256, reg: 0.013641)\n",
      "Iteration 700, Loss: 0.591210 (data: 0.575406, reg: 0.015804)\n",
      "Iteration 800, Loss: 0.571904 (data: 0.553958, reg: 0.017946)\n",
      "Iteration 900, Loss: 0.555169 (data: 0.535104, reg: 0.020065)\n",
      "Iteration 1000, Loss: 0.540459 (data: 0.518298, reg: 0.022161)\n",
      "Iteration 1100, Loss: 0.527389 (data: 0.503157, reg: 0.024233)\n",
      "Iteration 1200, Loss: 0.515677 (data: 0.489397, reg: 0.026279)\n",
      "Iteration 1300, Loss: 0.505108 (data: 0.476807, reg: 0.028301)\n",
      "Iteration 1400, Loss: 0.495517 (data: 0.465220, reg: 0.030298)\n",
      "Iteration 1500, Loss: 0.486772 (data: 0.454504, reg: 0.032268)\n",
      "Iteration 1600, Loss: 0.478765 (data: 0.444553, reg: 0.034212)\n",
      "Iteration 1700, Loss: 0.471409 (data: 0.435280, reg: 0.036130)\n",
      "Iteration 1800, Loss: 0.464630 (data: 0.426610, reg: 0.038020)\n",
      "Iteration 1900, Loss: 0.458364 (data: 0.418481, reg: 0.039883)\n",
      "LR: 0.01, Iter: 2000, C: 100.0, Val Acc: 0.7248\n",
      "Iteration 0, Loss: 229.238898 (data: 1.096099, reg: 228.142799)\n",
      "Iteration 100, Loss: inf (data: 17.976044, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 500, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.177828 (data: 1.106791, reg: 22.071038)\n",
      "Iteration 100, Loss: 57040317664178552946295235885290880991454711580789615301827124895226904388844635339231357544431029854810526769818378436608.000000 (data: 20.643866, reg: 57040317664178552946295235885290880991454711580789615301827124895226904388844635339231357544431029854810526769818378436608.000000)\n",
      "Iteration 200, Loss: 147292353334363606877506785230474679940132379206653425369089753542382860550882838194035034299714846510650053223505402345666996536871923637381567682815192588178705772794732160466153821151247010107790342212478144317915502602423438955401715908608.000000 (data: 20.643866, reg: 147292353334363606877506785230474679940132379206653425369089753542382860550882838194035034299714846510650053223505402345666996536871923637381567682815192588178705772794732160466153821151247010107790342212478144317915502602423438955401715908608.000000)\n",
      "Iteration 300, Loss: inf (data: 20.643866, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 20.643866, reg: inf)\n",
      "LR: 0.05, Iter: 500, C: 0.01, Val Acc: 0.2752\n",
      "Iteration 0, Loss: 3.453340 (data: 1.107134, reg: 2.346205)\n",
      "Iteration 100, Loss: 1.078867 (data: 1.063278, reg: 0.015589)\n",
      "Iteration 200, Loss: 1.078777 (data: 1.063416, reg: 0.015361)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063462, reg: 0.015310)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063474, reg: 0.015299)\n",
      "LR: 0.05, Iter: 500, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.324506 (data: 1.105345, reg: 0.219161)\n",
      "Iteration 100, Loss: 0.979671 (data: 0.899163, reg: 0.080508)\n",
      "Iteration 200, Loss: 0.979425 (data: 0.898027, reg: 0.081398)\n",
      "Iteration 300, Loss: 0.979350 (data: 0.897552, reg: 0.081798)\n",
      "Iteration 400, Loss: 0.979327 (data: 0.897294, reg: 0.082032)\n",
      "LR: 0.05, Iter: 500, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.121319 (data: 1.098260, reg: 0.023060)\n",
      "Iteration 100, Loss: 0.748864 (data: 0.674358, reg: 0.074506)\n",
      "Iteration 200, Loss: 0.716010 (data: 0.606745, reg: 0.109265)\n",
      "Iteration 300, Loss: 0.707671 (data: 0.580875, reg: 0.126796)\n",
      "Iteration 400, Loss: 0.704780 (data: 0.568937, reg: 0.135843)\n",
      "LR: 0.05, Iter: 500, C: 10.0, Val Acc: 0.7383\n",
      "Iteration 0, Loss: 1.107815 (data: 1.105487, reg: 0.002328)\n",
      "Iteration 100, Loss: 0.640675 (data: 0.629120, reg: 0.011555)\n",
      "Iteration 200, Loss: 0.540313 (data: 0.518072, reg: 0.022241)\n",
      "Iteration 300, Loss: 0.486757 (data: 0.454429, reg: 0.032328)\n",
      "Iteration 400, Loss: 0.452579 (data: 0.410815, reg: 0.041765)\n",
      "LR: 0.05, Iter: 500, C: 100.0, Val Acc: 0.7315\n",
      "Iteration 0, Loss: 220.472564 (data: 1.103132, reg: 219.369432)\n",
      "Iteration 100, Loss: inf (data: 18.865318, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.074719 (data: 1.096248, reg: 21.978471)\n",
      "Iteration 100, Loss: 56772071990981678503216719561619751955274239452154397037372317463379259779278302429808358792545941106963475529357726842880.000000 (data: 19.119396, reg: 56772071990981678503216719561619751955274239452154397037372317463379259779278302429808358792545941106963475529357726842880.000000)\n",
      "Iteration 200, Loss: 146599675977453637123747125552339099404480194468549750016544249606545725982199152077864917489744264392735683782675506027311896277762907700699828728074286269967991829722699203294250812707405256744782502204746927044457237411054258108830192762880.000000 (data: 19.119396, reg: 146599675977453637123747125552339099404480194468549750016544249606545725982199152077864917489744264392735683782675506027311896277762907700699828728074286269967991829722699203294250812707405256744782502204746927044457237411054258108830192762880.000000)\n",
      "Iteration 300, Loss: inf (data: 19.119396, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 19.119396, reg: inf)\n",
      "Iteration 500, Loss: inf (data: 19.119396, reg: inf)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.499699 (data: 1.099084, reg: 2.400614)\n",
      "Iteration 100, Loss: 1.078867 (data: 1.063278, reg: 0.015589)\n",
      "Iteration 200, Loss: 1.078777 (data: 1.063416, reg: 0.015361)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063462, reg: 0.015310)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063474, reg: 0.015299)\n",
      "Iteration 500, Loss: 1.078772 (data: 1.063476, reg: 0.015296)\n",
      "Iteration 600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.05, Iter: 1000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.329331 (data: 1.100645, reg: 0.228685)\n",
      "Iteration 100, Loss: 0.979682 (data: 0.899182, reg: 0.080500)\n",
      "Iteration 200, Loss: 0.979428 (data: 0.898043, reg: 0.081385)\n",
      "Iteration 300, Loss: 0.979351 (data: 0.897560, reg: 0.081790)\n",
      "Iteration 400, Loss: 0.979327 (data: 0.897299, reg: 0.082028)\n",
      "Iteration 500, Loss: 0.979319 (data: 0.897155, reg: 0.082164)\n",
      "Iteration 600, Loss: 0.979317 (data: 0.897076, reg: 0.082242)\n",
      "Iteration 700, Loss: 0.979316 (data: 0.897031, reg: 0.082285)\n",
      "Iteration 800, Loss: 0.979316 (data: 0.897006, reg: 0.082310)\n",
      "Iteration 900, Loss: 0.979316 (data: 0.896993, reg: 0.082323)\n",
      "LR: 0.05, Iter: 1000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.115269 (data: 1.091856, reg: 0.023413)\n",
      "Iteration 100, Loss: 0.748321 (data: 0.673165, reg: 0.075157)\n",
      "Iteration 200, Loss: 0.715899 (data: 0.606277, reg: 0.109622)\n",
      "Iteration 300, Loss: 0.707639 (data: 0.580660, reg: 0.126979)\n",
      "Iteration 400, Loss: 0.704766 (data: 0.568832, reg: 0.135934)\n",
      "Iteration 500, Loss: 0.703483 (data: 0.562817, reg: 0.140666)\n",
      "Iteration 600, Loss: 0.702771 (data: 0.559530, reg: 0.143241)\n",
      "Iteration 700, Loss: 0.702309 (data: 0.557610, reg: 0.144699)\n",
      "Iteration 800, Loss: 0.701984 (data: 0.556413, reg: 0.145571)\n",
      "Iteration 900, Loss: 0.701743 (data: 0.555614, reg: 0.146129)\n",
      "LR: 0.05, Iter: 1000, C: 10.0, Val Acc: 0.7584\n",
      "Iteration 0, Loss: 1.107911 (data: 1.105669, reg: 0.002241)\n",
      "Iteration 100, Loss: 0.641272 (data: 0.629838, reg: 0.011434)\n",
      "Iteration 200, Loss: 0.540542 (data: 0.518408, reg: 0.022135)\n",
      "Iteration 300, Loss: 0.486846 (data: 0.454609, reg: 0.032237)\n",
      "Iteration 400, Loss: 0.452603 (data: 0.410916, reg: 0.041687)\n",
      "Iteration 500, Loss: 0.429058 (data: 0.378607, reg: 0.050451)\n",
      "Iteration 600, Loss: 0.412145 (data: 0.353621, reg: 0.058524)\n",
      "Iteration 700, Loss: 0.399630 (data: 0.333707, reg: 0.065923)\n",
      "Iteration 800, Loss: 0.390168 (data: 0.317489, reg: 0.072680)\n",
      "Iteration 900, Loss: 0.382896 (data: 0.304062, reg: 0.078834)\n",
      "LR: 0.05, Iter: 1000, C: 100.0, Val Acc: 0.7383\n",
      "Iteration 0, Loss: 235.768268 (data: 1.102684, reg: 234.665584)\n",
      "Iteration 100, Loss: inf (data: 18.357161, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.674647 (data: 1.097285, reg: 22.577362)\n",
      "Iteration 100, Loss: 58324171115610454261529769149501108316896501970796472160338605260454799759880542755892545728754053650065129146117208932352.000000 (data: 18.928837, reg: 58324171115610454261529769149501108316896501970796472160338605260454799759880542755892545728754053650065129146117208932352.000000)\n",
      "Iteration 200, Loss: 150607583752805090866868028959099180438000601764463633977611277182958526277493816586750038620321936718543502180530327022918322413733698982646923346272258731905458381037681946469064223314197838911976570018160189355806555824995520234685152100352.000000 (data: 18.928837, reg: 150607583752805090866868028959099180438000601764463633977611277182958526277493816586750038620321936718543502180530327022918322413733698982646923346272258731905458381037681946469064223314197838911976570018160189355806555824995520234685152100352.000000)\n",
      "Iteration 300, Loss: inf (data: 18.928837, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 18.928837, reg: inf)\n",
      "Iteration 500, Loss: inf (data: 18.928837, reg: inf)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.361572 (data: 1.105559, reg: 2.256012)\n",
      "Iteration 100, Loss: 1.078865 (data: 1.063279, reg: 0.015585)\n",
      "Iteration 200, Loss: 1.078777 (data: 1.063417, reg: 0.015360)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063462, reg: 0.015310)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063474, reg: 0.015299)\n",
      "Iteration 500, Loss: 1.078772 (data: 1.063476, reg: 0.015296)\n",
      "Iteration 600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1000, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1100, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1200, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1500, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.05, Iter: 2000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.334429 (data: 1.107084, reg: 0.227345)\n",
      "Iteration 100, Loss: 0.979713 (data: 0.899256, reg: 0.080457)\n",
      "Iteration 200, Loss: 0.979438 (data: 0.898090, reg: 0.081347)\n",
      "Iteration 300, Loss: 0.979354 (data: 0.897586, reg: 0.081768)\n",
      "Iteration 400, Loss: 0.979328 (data: 0.897313, reg: 0.082015)\n",
      "Iteration 500, Loss: 0.979320 (data: 0.897163, reg: 0.082157)\n",
      "Iteration 600, Loss: 0.979317 (data: 0.897080, reg: 0.082237)\n",
      "Iteration 700, Loss: 0.979316 (data: 0.897034, reg: 0.082283)\n",
      "Iteration 800, Loss: 0.979316 (data: 0.897008, reg: 0.082308)\n",
      "Iteration 900, Loss: 0.979316 (data: 0.896993, reg: 0.082323)\n",
      "Iteration 1000, Loss: 0.979316 (data: 0.896985, reg: 0.082331)\n",
      "Iteration 1100, Loss: 0.979316 (data: 0.896981, reg: 0.082335)\n",
      "Iteration 1200, Loss: 0.979316 (data: 0.896978, reg: 0.082338)\n",
      "Iteration 1300, Loss: 0.979316 (data: 0.896977, reg: 0.082339)\n",
      "Iteration 1400, Loss: 0.979316 (data: 0.896976, reg: 0.082340)\n",
      "Iteration 1500, Loss: 0.979316 (data: 0.896976, reg: 0.082340)\n",
      "Iteration 1600, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1700, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1800, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1900, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "LR: 0.05, Iter: 2000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.122420 (data: 1.099702, reg: 0.022717)\n",
      "Iteration 100, Loss: 0.748243 (data: 0.673499, reg: 0.074744)\n",
      "Iteration 200, Loss: 0.715868 (data: 0.606420, reg: 0.109449)\n",
      "Iteration 300, Loss: 0.707640 (data: 0.580748, reg: 0.126892)\n",
      "Iteration 400, Loss: 0.704775 (data: 0.568890, reg: 0.135885)\n",
      "Iteration 500, Loss: 0.703493 (data: 0.562856, reg: 0.140637)\n",
      "Iteration 600, Loss: 0.702779 (data: 0.559557, reg: 0.143222)\n",
      "Iteration 700, Loss: 0.702316 (data: 0.557630, reg: 0.144686)\n",
      "Iteration 800, Loss: 0.701989 (data: 0.556427, reg: 0.145562)\n",
      "Iteration 900, Loss: 0.701748 (data: 0.555625, reg: 0.146123)\n",
      "Iteration 1000, Loss: 0.701566 (data: 0.555057, reg: 0.146509)\n",
      "Iteration 1100, Loss: 0.701428 (data: 0.554631, reg: 0.146797)\n",
      "Iteration 1200, Loss: 0.701323 (data: 0.554300, reg: 0.147023)\n",
      "Iteration 1300, Loss: 0.701242 (data: 0.554032, reg: 0.147210)\n",
      "Iteration 1400, Loss: 0.701180 (data: 0.553811, reg: 0.147369)\n",
      "Iteration 1500, Loss: 0.701133 (data: 0.553625, reg: 0.147507)\n",
      "Iteration 1600, Loss: 0.701096 (data: 0.553468, reg: 0.147628)\n",
      "Iteration 1700, Loss: 0.701068 (data: 0.553333, reg: 0.147735)\n",
      "Iteration 1800, Loss: 0.701047 (data: 0.553217, reg: 0.147829)\n",
      "Iteration 1900, Loss: 0.701030 (data: 0.553117, reg: 0.147913)\n",
      "LR: 0.05, Iter: 2000, C: 10.0, Val Acc: 0.7517\n",
      "Iteration 0, Loss: 1.102025 (data: 1.099757, reg: 0.002268)\n",
      "Iteration 100, Loss: 0.638190 (data: 0.626528, reg: 0.011662)\n",
      "Iteration 200, Loss: 0.538695 (data: 0.516268, reg: 0.022427)\n",
      "Iteration 300, Loss: 0.485596 (data: 0.453037, reg: 0.032559)\n",
      "Iteration 400, Loss: 0.451710 (data: 0.409691, reg: 0.042019)\n",
      "Iteration 500, Loss: 0.428399 (data: 0.377616, reg: 0.050782)\n",
      "Iteration 600, Loss: 0.411647 (data: 0.352799, reg: 0.058848)\n",
      "Iteration 700, Loss: 0.399249 (data: 0.333014, reg: 0.066235)\n",
      "Iteration 800, Loss: 0.389874 (data: 0.316896, reg: 0.072978)\n",
      "Iteration 900, Loss: 0.382668 (data: 0.303550, reg: 0.079118)\n",
      "Iteration 1000, Loss: 0.377056 (data: 0.292358, reg: 0.084698)\n",
      "Iteration 1100, Loss: 0.372635 (data: 0.282874, reg: 0.089762)\n",
      "Iteration 1200, Loss: 0.369121 (data: 0.274769, reg: 0.094352)\n",
      "Iteration 1300, Loss: 0.366303 (data: 0.267794, reg: 0.098510)\n",
      "Iteration 1400, Loss: 0.364026 (data: 0.261754, reg: 0.102273)\n",
      "Iteration 1500, Loss: 0.362173 (data: 0.256496, reg: 0.105678)\n",
      "Iteration 1600, Loss: 0.360655 (data: 0.251898, reg: 0.108757)\n",
      "Iteration 1700, Loss: 0.359404 (data: 0.247861, reg: 0.111543)\n",
      "Iteration 1800, Loss: 0.358365 (data: 0.244304, reg: 0.114061)\n",
      "Iteration 1900, Loss: 0.357499 (data: 0.241159, reg: 0.116339)\n",
      "LR: 0.05, Iter: 2000, C: 100.0, Val Acc: 0.7114\n",
      "Iteration 0, Loss: 226.811299 (data: 1.102743, reg: 225.708556)\n",
      "Iteration 100, Loss: inf (data: 18.738279, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 500, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 24.063379 (data: 1.103116, reg: 22.960262)\n",
      "Iteration 100, Loss: 1620714487970334702779975291289170405139668561908898546601780596922236762350281685927385934222109343577399109034627080682741931274466787655511293156799967555458953075438633172727087808215252992.000000 (data: 19.182916, reg: 1620714487970334702779975291289170405139668561908898546601780596922236762350281685927385934222109343577399109034627080682741931274466787655511293156799967555458953075438633172727087808215252992.000000)\n",
      "Iteration 200, Loss: inf (data: 19.182916, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 19.182916, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 500, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.437881 (data: 1.100515, reg: 2.337366)\n",
      "Iteration 100, Loss: 1.078777 (data: 1.063417, reg: 0.015360)\n",
      "Iteration 200, Loss: 1.078772 (data: 1.063474, reg: 0.015298)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.1, Iter: 500, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.322011 (data: 1.090818, reg: 0.231193)\n",
      "Iteration 100, Loss: 0.979430 (data: 0.898054, reg: 0.081376)\n",
      "Iteration 200, Loss: 0.979327 (data: 0.897301, reg: 0.082026)\n",
      "Iteration 300, Loss: 0.979317 (data: 0.897076, reg: 0.082241)\n",
      "Iteration 400, Loss: 0.979316 (data: 0.897006, reg: 0.082310)\n",
      "LR: 0.1, Iter: 500, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.118058 (data: 1.095435, reg: 0.022623)\n",
      "Iteration 100, Loss: 0.715872 (data: 0.606412, reg: 0.109460)\n",
      "Iteration 200, Loss: 0.704754 (data: 0.568855, reg: 0.135899)\n",
      "Iteration 300, Loss: 0.702767 (data: 0.559524, reg: 0.143243)\n",
      "Iteration 400, Loss: 0.701983 (data: 0.556404, reg: 0.145579)\n",
      "LR: 0.1, Iter: 500, C: 10.0, Val Acc: 0.7584\n",
      "Iteration 0, Loss: 1.097423 (data: 1.095133, reg: 0.002289)\n",
      "Iteration 100, Loss: 0.539959 (data: 0.517681, reg: 0.022278)\n",
      "Iteration 200, Loss: 0.452467 (data: 0.410671, reg: 0.041796)\n",
      "Iteration 300, Loss: 0.412137 (data: 0.353539, reg: 0.058598)\n",
      "Iteration 400, Loss: 0.390192 (data: 0.317466, reg: 0.072726)\n",
      "LR: 0.1, Iter: 500, C: 100.0, Val Acc: 0.7315\n",
      "Iteration 0, Loss: 237.008528 (data: 1.103676, reg: 235.904852)\n",
      "Iteration 100, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 24.788135 (data: 1.106638, reg: 23.681497)\n",
      "Iteration 100, Loss: 1672079768243604476175408623474254665040958874447200590972556322927662498319188732373480025442892940534187589687421618923781735311815989775548668802551897999525182514272058487230308824089886720.000000 (data: 19.691073, reg: 1672079768243604476175408623474254665040958874447200590972556322927662498319188732373480025442892940534187589687421618923781735311815989775548668802551897999525182514272058487230308824089886720.000000)\n",
      "Iteration 200, Loss: inf (data: 19.691073, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 19.691073, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.309714 (data: 1.101384, reg: 2.208330)\n",
      "Iteration 100, Loss: 1.078777 (data: 1.063417, reg: 0.015360)\n",
      "Iteration 200, Loss: 1.078772 (data: 1.063474, reg: 0.015299)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 500, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.1, Iter: 1000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.335510 (data: 1.102137, reg: 0.233373)\n",
      "Iteration 100, Loss: 0.979442 (data: 0.898112, reg: 0.081330)\n",
      "Iteration 200, Loss: 0.979328 (data: 0.897318, reg: 0.082010)\n",
      "Iteration 300, Loss: 0.979317 (data: 0.897081, reg: 0.082236)\n",
      "Iteration 400, Loss: 0.979316 (data: 0.897008, reg: 0.082308)\n",
      "Iteration 500, Loss: 0.979316 (data: 0.896985, reg: 0.082331)\n",
      "Iteration 600, Loss: 0.979316 (data: 0.896978, reg: 0.082338)\n",
      "Iteration 700, Loss: 0.979316 (data: 0.896976, reg: 0.082340)\n",
      "Iteration 800, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 900, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "LR: 0.1, Iter: 1000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.132346 (data: 1.109552, reg: 0.022795)\n",
      "Iteration 100, Loss: 0.715677 (data: 0.606014, reg: 0.109663)\n",
      "Iteration 200, Loss: 0.704708 (data: 0.568705, reg: 0.136003)\n",
      "Iteration 300, Loss: 0.702747 (data: 0.559470, reg: 0.143277)\n",
      "Iteration 400, Loss: 0.701971 (data: 0.556381, reg: 0.145590)\n",
      "Iteration 500, Loss: 0.701555 (data: 0.555029, reg: 0.146526)\n",
      "Iteration 600, Loss: 0.701316 (data: 0.554281, reg: 0.147035)\n",
      "Iteration 700, Loss: 0.701176 (data: 0.553798, reg: 0.147379)\n",
      "Iteration 800, Loss: 0.701094 (data: 0.553458, reg: 0.147636)\n",
      "Iteration 900, Loss: 0.701045 (data: 0.553210, reg: 0.147836)\n",
      "LR: 0.1, Iter: 1000, C: 10.0, Val Acc: 0.7517\n",
      "Iteration 0, Loss: 1.106789 (data: 1.104567, reg: 0.002222)\n",
      "Iteration 100, Loss: 0.539818 (data: 0.517584, reg: 0.022233)\n",
      "Iteration 200, Loss: 0.452356 (data: 0.410581, reg: 0.041775)\n",
      "Iteration 300, Loss: 0.412051 (data: 0.353465, reg: 0.058586)\n",
      "Iteration 400, Loss: 0.390127 (data: 0.317407, reg: 0.072720)\n",
      "Iteration 500, Loss: 0.377209 (data: 0.292755, reg: 0.084454)\n",
      "Iteration 600, Loss: 0.369210 (data: 0.275081, reg: 0.094129)\n",
      "Iteration 700, Loss: 0.364072 (data: 0.261999, reg: 0.102073)\n",
      "Iteration 800, Loss: 0.360673 (data: 0.252093, reg: 0.108580)\n",
      "Iteration 900, Loss: 0.358365 (data: 0.244458, reg: 0.113907)\n",
      "LR: 0.1, Iter: 1000, C: 100.0, Val Acc: 0.7181\n",
      "Iteration 0, Loss: 225.868887 (data: 1.108563, reg: 224.760324)\n",
      "Iteration 100, Loss: inf (data: 20.262749, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 22.905232 (data: 1.101384, reg: 21.803848)\n",
      "Iteration 100, Loss: 1538959829638174985242910303130000624941981466718974959598702052749998713595498076402087587751849490264731909269672830421367308614761237404503543674630430053143292282474542069901945313488273408.000000 (data: 18.865318, reg: 1538959829638174985242910303130000624941981466718974959598702052749998713595498076402087587751849490264731909269672830421367308614761237404503543674630430053143292282474542069901945313488273408.000000)\n",
      "Iteration 200, Loss: inf (data: 18.865318, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 18.865318, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.404775 (data: 1.084070, reg: 2.320705)\n",
      "Iteration 100, Loss: 1.078777 (data: 1.063417, reg: 0.015360)\n",
      "Iteration 200, Loss: 1.078772 (data: 1.063474, reg: 0.015298)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 500, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1000, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1100, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1200, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1500, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.1, Iter: 2000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.325899 (data: 1.097116, reg: 0.228783)\n",
      "Iteration 100, Loss: 0.979430 (data: 0.898054, reg: 0.081376)\n",
      "Iteration 200, Loss: 0.979327 (data: 0.897301, reg: 0.082026)\n",
      "Iteration 300, Loss: 0.979317 (data: 0.897076, reg: 0.082241)\n",
      "Iteration 400, Loss: 0.979316 (data: 0.897006, reg: 0.082310)\n",
      "Iteration 500, Loss: 0.979316 (data: 0.896985, reg: 0.082331)\n",
      "Iteration 600, Loss: 0.979316 (data: 0.896978, reg: 0.082338)\n",
      "Iteration 700, Loss: 0.979316 (data: 0.896976, reg: 0.082340)\n",
      "Iteration 800, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 900, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1000, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1100, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1200, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1300, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1400, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1500, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1600, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1700, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1800, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1900, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "LR: 0.1, Iter: 2000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.120717 (data: 1.098439, reg: 0.022278)\n",
      "Iteration 100, Loss: 0.715772 (data: 0.606184, reg: 0.109588)\n",
      "Iteration 200, Loss: 0.704783 (data: 0.568816, reg: 0.135967)\n",
      "Iteration 300, Loss: 0.702793 (data: 0.559545, reg: 0.143247)\n",
      "Iteration 400, Loss: 0.701998 (data: 0.556434, reg: 0.145564)\n",
      "Iteration 500, Loss: 0.701571 (data: 0.555067, reg: 0.146504)\n",
      "Iteration 600, Loss: 0.701325 (data: 0.554308, reg: 0.147017)\n",
      "Iteration 700, Loss: 0.701182 (data: 0.553817, reg: 0.147364)\n",
      "Iteration 800, Loss: 0.701097 (data: 0.553473, reg: 0.147624)\n",
      "Iteration 900, Loss: 0.701047 (data: 0.553221, reg: 0.147826)\n",
      "Iteration 1000, Loss: 0.701018 (data: 0.553033, reg: 0.147985)\n",
      "Iteration 1100, Loss: 0.701000 (data: 0.552892, reg: 0.148108)\n",
      "Iteration 1200, Loss: 0.700990 (data: 0.552785, reg: 0.148205)\n",
      "Iteration 1300, Loss: 0.700984 (data: 0.552704, reg: 0.148280)\n",
      "Iteration 1400, Loss: 0.700980 (data: 0.552642, reg: 0.148338)\n",
      "Iteration 1500, Loss: 0.700978 (data: 0.552595, reg: 0.148383)\n",
      "Iteration 1600, Loss: 0.700977 (data: 0.552559, reg: 0.148418)\n",
      "Iteration 1700, Loss: 0.700976 (data: 0.552531, reg: 0.148445)\n",
      "Iteration 1800, Loss: 0.700976 (data: 0.552510, reg: 0.148465)\n",
      "Iteration 1900, Loss: 0.700975 (data: 0.552494, reg: 0.148481)\n",
      "LR: 0.1, Iter: 2000, C: 10.0, Val Acc: 0.7584\n",
      "Iteration 0, Loss: 1.103240 (data: 1.100883, reg: 0.002357)\n",
      "Iteration 100, Loss: 0.538985 (data: 0.516515, reg: 0.022470)\n",
      "Iteration 200, Loss: 0.452009 (data: 0.410015, reg: 0.041993)\n",
      "Iteration 300, Loss: 0.411875 (data: 0.353093, reg: 0.058783)\n",
      "Iteration 400, Loss: 0.390036 (data: 0.317143, reg: 0.072893)\n",
      "Iteration 500, Loss: 0.377166 (data: 0.292561, reg: 0.084605)\n",
      "Iteration 600, Loss: 0.369194 (data: 0.274935, reg: 0.094259)\n",
      "Iteration 700, Loss: 0.364072 (data: 0.261888, reg: 0.102184)\n",
      "Iteration 800, Loss: 0.360682 (data: 0.252007, reg: 0.108674)\n",
      "Iteration 900, Loss: 0.358378 (data: 0.244392, reg: 0.113986)\n",
      "Iteration 1000, Loss: 0.356775 (data: 0.238443, reg: 0.118332)\n",
      "Iteration 1100, Loss: 0.355634 (data: 0.233744, reg: 0.121890)\n",
      "Iteration 1200, Loss: 0.354804 (data: 0.229997, reg: 0.124807)\n",
      "Iteration 1300, Loss: 0.354187 (data: 0.226986, reg: 0.127202)\n",
      "Iteration 1400, Loss: 0.353720 (data: 0.224549, reg: 0.129171)\n",
      "Iteration 1500, Loss: 0.353359 (data: 0.222565, reg: 0.130795)\n",
      "Iteration 1600, Loss: 0.353076 (data: 0.220940, reg: 0.132135)\n",
      "Iteration 1700, Loss: 0.352848 (data: 0.219603, reg: 0.133246)\n",
      "Iteration 1800, Loss: 0.352664 (data: 0.218497, reg: 0.134167)\n",
      "Iteration 1900, Loss: 0.352512 (data: 0.217577, reg: 0.134935)\n",
      "LR: 0.1, Iter: 2000, C: 100.0, Val Acc: 0.7181\n",
      "Best Validation Accuracy: 0.7584 with params: {'learning_rate': 0.05, 'num_iterations': 1000, 'C': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05, 'num_iterations': 1000, 'C': 10.0}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_hyperparameters(X_train, y_train_one, X_valid, y_valid_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "07fb7de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 227.192413 (data: 1.098565, reg: 226.093848)\n",
      "Iteration 100, Loss: 1.098169 (data: 1.097973, reg: 0.000196)\n",
      "Iteration 200, Loss: 1.097939 (data: 1.097744, reg: 0.000195)\n",
      "Iteration 300, Loss: 1.097723 (data: 1.097530, reg: 0.000194)\n",
      "Iteration 400, Loss: 1.097522 (data: 1.097329, reg: 0.000193)\n",
      "LR: 0.001, Iter: 500, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.851482 (data: 1.102282, reg: 22.749200)\n",
      "Iteration 100, Loss: 1.096432 (data: 1.094505, reg: 0.001927)\n",
      "Iteration 200, Loss: 1.096220 (data: 1.094304, reg: 0.001917)\n",
      "Iteration 300, Loss: 1.096022 (data: 1.094115, reg: 0.001907)\n",
      "Iteration 400, Loss: 1.095836 (data: 1.093939, reg: 0.001897)\n",
      "LR: 0.001, Iter: 500, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.313281 (data: 1.094921, reg: 2.218360)\n",
      "Iteration 100, Loss: 1.378691 (data: 1.073257, reg: 0.305434)\n",
      "Iteration 200, Loss: 1.120135 (data: 1.066731, reg: 0.053404)\n",
      "Iteration 300, Loss: 1.085497 (data: 1.064607, reg: 0.020890)\n",
      "Iteration 400, Loss: 1.080798 (data: 1.063882, reg: 0.016916)\n",
      "LR: 0.001, Iter: 500, C: 0.1, Val Acc: 0.4966\n",
      "Iteration 0, Loss: 1.334871 (data: 1.106274, reg: 0.228597)\n",
      "Iteration 100, Loss: 1.258747 (data: 1.070720, reg: 0.188027)\n",
      "Iteration 200, Loss: 1.200579 (data: 1.043130, reg: 0.157449)\n",
      "Iteration 300, Loss: 1.155498 (data: 1.021140, reg: 0.134359)\n",
      "Iteration 400, Loss: 1.120192 (data: 1.003247, reg: 0.116945)\n",
      "LR: 0.001, Iter: 500, C: 1.0, Val Acc: 0.5369\n",
      "Iteration 0, Loss: 1.127147 (data: 1.104511, reg: 0.022636)\n",
      "Iteration 100, Loss: 1.089773 (data: 1.067468, reg: 0.022305)\n",
      "Iteration 200, Loss: 1.058913 (data: 1.036624, reg: 0.022289)\n",
      "Iteration 300, Loss: 1.032743 (data: 1.010230, reg: 0.022514)\n",
      "Iteration 400, Loss: 1.010053 (data: 0.987127, reg: 0.022926)\n",
      "LR: 0.001, Iter: 500, C: 10.0, Val Acc: 0.5638\n",
      "Iteration 0, Loss: 1.109745 (data: 1.107540, reg: 0.002205)\n",
      "Iteration 100, Loss: 1.073236 (data: 1.071028, reg: 0.002208)\n",
      "Iteration 200, Loss: 1.042400 (data: 1.040157, reg: 0.002243)\n",
      "Iteration 300, Loss: 1.015677 (data: 1.013376, reg: 0.002301)\n",
      "Iteration 400, Loss: 0.992033 (data: 0.989654, reg: 0.002379)\n",
      "LR: 0.001, Iter: 500, C: 100.0, Val Acc: 0.5638\n",
      "Iteration 0, Loss: 229.642466 (data: 1.092374, reg: 228.550092)\n",
      "Iteration 100, Loss: 1.098170 (data: 1.097974, reg: 0.000196)\n",
      "Iteration 200, Loss: 1.097939 (data: 1.097745, reg: 0.000195)\n",
      "Iteration 300, Loss: 1.097724 (data: 1.097530, reg: 0.000194)\n",
      "Iteration 400, Loss: 1.097523 (data: 1.097330, reg: 0.000193)\n",
      "Iteration 500, Loss: 1.097334 (data: 1.097143, reg: 0.000192)\n",
      "Iteration 600, Loss: 1.097158 (data: 1.096967, reg: 0.000191)\n",
      "Iteration 700, Loss: 1.096993 (data: 1.096803, reg: 0.000190)\n",
      "Iteration 800, Loss: 1.096839 (data: 1.096650, reg: 0.000189)\n",
      "Iteration 900, Loss: 1.096695 (data: 1.096507, reg: 0.000188)\n",
      "LR: 0.001, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.835017 (data: 1.103387, reg: 22.731630)\n",
      "Iteration 100, Loss: 1.096426 (data: 1.094500, reg: 0.001927)\n",
      "Iteration 200, Loss: 1.096215 (data: 1.094298, reg: 0.001917)\n",
      "Iteration 300, Loss: 1.096017 (data: 1.094110, reg: 0.001907)\n",
      "Iteration 400, Loss: 1.095832 (data: 1.093935, reg: 0.001897)\n",
      "Iteration 500, Loss: 1.095658 (data: 1.093770, reg: 0.001888)\n",
      "Iteration 600, Loss: 1.095496 (data: 1.093617, reg: 0.001879)\n",
      "Iteration 700, Loss: 1.095344 (data: 1.093473, reg: 0.001871)\n",
      "Iteration 800, Loss: 1.095201 (data: 1.093339, reg: 0.001863)\n",
      "Iteration 900, Loss: 1.095068 (data: 1.093213, reg: 0.001855)\n",
      "LR: 0.001, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.348340 (data: 1.100913, reg: 2.247428)\n",
      "Iteration 100, Loss: 1.383412 (data: 1.075490, reg: 0.307922)\n",
      "Iteration 200, Loss: 1.120773 (data: 1.067542, reg: 0.053232)\n",
      "Iteration 300, Loss: 1.085592 (data: 1.064901, reg: 0.020690)\n",
      "Iteration 400, Loss: 1.080820 (data: 1.063990, reg: 0.016831)\n",
      "Iteration 500, Loss: 1.080120 (data: 1.063659, reg: 0.016460)\n",
      "Iteration 600, Loss: 1.079966 (data: 1.063528, reg: 0.016439)\n",
      "Iteration 700, Loss: 1.079890 (data: 1.063466, reg: 0.016424)\n",
      "Iteration 800, Loss: 1.079827 (data: 1.063429, reg: 0.016397)\n",
      "Iteration 900, Loss: 1.079768 (data: 1.063403, reg: 0.016366)\n",
      "LR: 0.001, Iter: 1000, C: 0.1, Val Acc: 0.4698\n",
      "Iteration 0, Loss: 1.331538 (data: 1.098174, reg: 0.233365)\n",
      "Iteration 100, Loss: 1.255444 (data: 1.062768, reg: 0.192676)\n",
      "Iteration 200, Loss: 1.197855 (data: 1.035903, reg: 0.161951)\n",
      "Iteration 300, Loss: 1.153457 (data: 1.014814, reg: 0.138643)\n",
      "Iteration 400, Loss: 1.118759 (data: 0.997811, reg: 0.120948)\n",
      "Iteration 500, Loss: 1.091378 (data: 0.983832, reg: 0.107546)\n",
      "Iteration 600, Loss: 1.069626 (data: 0.972176, reg: 0.097450)\n",
      "Iteration 700, Loss: 1.052264 (data: 0.962358, reg: 0.089906)\n",
      "Iteration 800, Loss: 1.038358 (data: 0.954025, reg: 0.084333)\n",
      "Iteration 900, Loss: 1.027191 (data: 0.946910, reg: 0.080280)\n",
      "LR: 0.001, Iter: 1000, C: 1.0, Val Acc: 0.6242\n",
      "Iteration 0, Loss: 1.125957 (data: 1.104098, reg: 0.021860)\n",
      "Iteration 100, Loss: 1.089450 (data: 1.067912, reg: 0.021539)\n",
      "Iteration 200, Loss: 1.059162 (data: 1.037638, reg: 0.021523)\n",
      "Iteration 300, Loss: 1.033374 (data: 1.011632, reg: 0.021742)\n",
      "Iteration 400, Loss: 1.010949 (data: 0.988803, reg: 0.022146)\n",
      "Iteration 500, Loss: 0.991130 (data: 0.968432, reg: 0.022698)\n",
      "Iteration 600, Loss: 0.973404 (data: 0.950030, reg: 0.023375)\n",
      "Iteration 700, Loss: 0.957409 (data: 0.933255, reg: 0.024154)\n",
      "Iteration 800, Loss: 0.942879 (data: 0.917859, reg: 0.025021)\n",
      "Iteration 900, Loss: 0.929611 (data: 0.903651, reg: 0.025961)\n",
      "LR: 0.001, Iter: 1000, C: 10.0, Val Acc: 0.6174\n",
      "Iteration 0, Loss: 1.092515 (data: 1.090283, reg: 0.002233)\n",
      "Iteration 100, Loss: 1.057860 (data: 1.055608, reg: 0.002251)\n",
      "Iteration 200, Loss: 1.028567 (data: 1.026268, reg: 0.002298)\n",
      "Iteration 300, Loss: 1.003127 (data: 1.000759, reg: 0.002368)\n",
      "Iteration 400, Loss: 0.980559 (data: 0.978104, reg: 0.002455)\n",
      "Iteration 500, Loss: 0.960222 (data: 0.957664, reg: 0.002558)\n",
      "Iteration 600, Loss: 0.941685 (data: 0.939010, reg: 0.002674)\n",
      "Iteration 700, Loss: 0.924645 (data: 0.921843, reg: 0.002802)\n",
      "Iteration 800, Loss: 0.908883 (data: 0.905944, reg: 0.002940)\n",
      "Iteration 900, Loss: 0.894232 (data: 0.891146, reg: 0.003086)\n",
      "LR: 0.001, Iter: 1000, C: 100.0, Val Acc: 0.6174\n",
      "Iteration 0, Loss: 228.344417 (data: 1.113657, reg: 227.230759)\n",
      "Iteration 100, Loss: 1.098169 (data: 1.097973, reg: 0.000196)\n",
      "Iteration 200, Loss: 1.097939 (data: 1.097744, reg: 0.000195)\n",
      "Iteration 300, Loss: 1.097723 (data: 1.097530, reg: 0.000194)\n",
      "Iteration 400, Loss: 1.097522 (data: 1.097329, reg: 0.000193)\n",
      "Iteration 500, Loss: 1.097333 (data: 1.097142, reg: 0.000192)\n",
      "Iteration 600, Loss: 1.097157 (data: 1.096967, reg: 0.000191)\n",
      "Iteration 700, Loss: 1.096993 (data: 1.096803, reg: 0.000190)\n",
      "Iteration 800, Loss: 1.096839 (data: 1.096650, reg: 0.000189)\n",
      "Iteration 900, Loss: 1.096695 (data: 1.096507, reg: 0.000188)\n",
      "Iteration 1000, Loss: 1.096560 (data: 1.096373, reg: 0.000187)\n",
      "Iteration 1100, Loss: 1.096435 (data: 1.096248, reg: 0.000187)\n",
      "Iteration 1200, Loss: 1.096317 (data: 1.096131, reg: 0.000186)\n",
      "Iteration 1300, Loss: 1.096207 (data: 1.096022, reg: 0.000185)\n",
      "Iteration 1400, Loss: 1.096105 (data: 1.095920, reg: 0.000184)\n",
      "Iteration 1500, Loss: 1.096009 (data: 1.095825, reg: 0.000184)\n",
      "Iteration 1600, Loss: 1.095920 (data: 1.095736, reg: 0.000183)\n",
      "Iteration 1700, Loss: 1.095836 (data: 1.095653, reg: 0.000183)\n",
      "Iteration 1800, Loss: 1.095758 (data: 1.095576, reg: 0.000182)\n",
      "Iteration 1900, Loss: 1.095685 (data: 1.095503, reg: 0.000182)\n",
      "LR: 0.001, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.927387 (data: 1.100787, reg: 22.826600)\n",
      "Iteration 100, Loss: 1.096437 (data: 1.094510, reg: 0.001927)\n",
      "Iteration 200, Loss: 1.096225 (data: 1.094308, reg: 0.001917)\n",
      "Iteration 300, Loss: 1.096026 (data: 1.094119, reg: 0.001907)\n",
      "Iteration 400, Loss: 1.095840 (data: 1.093943, reg: 0.001897)\n",
      "Iteration 500, Loss: 1.095666 (data: 1.093778, reg: 0.001888)\n",
      "Iteration 600, Loss: 1.095503 (data: 1.093624, reg: 0.001880)\n",
      "Iteration 700, Loss: 1.095351 (data: 1.093480, reg: 0.001871)\n",
      "Iteration 800, Loss: 1.095208 (data: 1.093345, reg: 0.001863)\n",
      "Iteration 900, Loss: 1.095074 (data: 1.093219, reg: 0.001856)\n",
      "Iteration 1000, Loss: 1.094949 (data: 1.093101, reg: 0.001848)\n",
      "Iteration 1100, Loss: 1.094833 (data: 1.092991, reg: 0.001841)\n",
      "Iteration 1200, Loss: 1.094723 (data: 1.092889, reg: 0.001835)\n",
      "Iteration 1300, Loss: 1.094621 (data: 1.092793, reg: 0.001828)\n",
      "Iteration 1400, Loss: 1.094525 (data: 1.092703, reg: 0.001822)\n",
      "Iteration 1500, Loss: 1.094435 (data: 1.092619, reg: 0.001816)\n",
      "Iteration 1600, Loss: 1.094352 (data: 1.092541, reg: 0.001810)\n",
      "Iteration 1700, Loss: 1.094273 (data: 1.092468, reg: 0.001805)\n",
      "Iteration 1800, Loss: 1.094200 (data: 1.092400, reg: 0.001800)\n",
      "Iteration 1900, Loss: 1.094132 (data: 1.092337, reg: 0.001795)\n",
      "LR: 0.001, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.447076 (data: 1.104082, reg: 2.342994)\n",
      "Iteration 100, Loss: 1.396712 (data: 1.076800, reg: 0.319912)\n",
      "Iteration 200, Loss: 1.122568 (data: 1.068047, reg: 0.054521)\n",
      "Iteration 300, Loss: 1.085840 (data: 1.065093, reg: 0.020747)\n",
      "Iteration 400, Loss: 1.080861 (data: 1.064063, reg: 0.016798)\n",
      "Iteration 500, Loss: 1.080132 (data: 1.063689, reg: 0.016443)\n",
      "Iteration 600, Loss: 1.079975 (data: 1.063541, reg: 0.016434)\n",
      "Iteration 700, Loss: 1.079898 (data: 1.063472, reg: 0.016425)\n",
      "Iteration 800, Loss: 1.079834 (data: 1.063433, reg: 0.016400)\n",
      "Iteration 900, Loss: 1.079775 (data: 1.063406, reg: 0.016369)\n",
      "Iteration 1000, Loss: 1.079720 (data: 1.063384, reg: 0.016336)\n",
      "Iteration 1100, Loss: 1.079667 (data: 1.063364, reg: 0.016303)\n",
      "Iteration 1200, Loss: 1.079618 (data: 1.063347, reg: 0.016271)\n",
      "Iteration 1300, Loss: 1.079571 (data: 1.063331, reg: 0.016239)\n",
      "Iteration 1400, Loss: 1.079527 (data: 1.063317, reg: 0.016209)\n",
      "Iteration 1500, Loss: 1.079485 (data: 1.063305, reg: 0.016180)\n",
      "Iteration 1600, Loss: 1.079445 (data: 1.063294, reg: 0.016152)\n",
      "Iteration 1700, Loss: 1.079408 (data: 1.063284, reg: 0.016125)\n",
      "Iteration 1800, Loss: 1.079373 (data: 1.063275, reg: 0.016098)\n",
      "Iteration 1900, Loss: 1.079340 (data: 1.063267, reg: 0.016073)\n",
      "LR: 0.001, Iter: 2000, C: 0.1, Val Acc: 0.4094\n",
      "Iteration 0, Loss: 1.343557 (data: 1.105850, reg: 0.237707)\n",
      "Iteration 100, Loss: 1.263565 (data: 1.067893, reg: 0.195672)\n",
      "Iteration 200, Loss: 1.203634 (data: 1.039568, reg: 0.164066)\n",
      "Iteration 300, Loss: 1.157758 (data: 1.017608, reg: 0.140150)\n",
      "Iteration 400, Loss: 1.122067 (data: 1.000054, reg: 0.122013)\n",
      "Iteration 500, Loss: 1.093980 (data: 0.985698, reg: 0.108282)\n",
      "Iteration 600, Loss: 1.071703 (data: 0.973767, reg: 0.097936)\n",
      "Iteration 700, Loss: 1.053937 (data: 0.963736, reg: 0.090200)\n",
      "Iteration 800, Loss: 1.039713 (data: 0.955232, reg: 0.084482)\n",
      "Iteration 900, Loss: 1.028294 (data: 0.947975, reg: 0.080318)\n",
      "Iteration 1000, Loss: 1.019105 (data: 0.941753, reg: 0.077352)\n",
      "Iteration 1100, Loss: 1.011698 (data: 0.936397, reg: 0.075301)\n",
      "Iteration 1200, Loss: 1.005719 (data: 0.931770, reg: 0.073948)\n",
      "Iteration 1300, Loss: 1.000885 (data: 0.927763, reg: 0.073122)\n",
      "Iteration 1400, Loss: 0.996972 (data: 0.924284, reg: 0.072689)\n",
      "Iteration 1500, Loss: 0.993802 (data: 0.921256, reg: 0.072546)\n",
      "Iteration 1600, Loss: 0.991231 (data: 0.918617, reg: 0.072614)\n",
      "Iteration 1700, Loss: 0.989144 (data: 0.916313, reg: 0.072831)\n",
      "Iteration 1800, Loss: 0.987447 (data: 0.914298, reg: 0.073149)\n",
      "Iteration 1900, Loss: 0.986067 (data: 0.912533, reg: 0.073534)\n",
      "LR: 0.001, Iter: 2000, C: 1.0, Val Acc: 0.6309\n",
      "Iteration 0, Loss: 1.114304 (data: 1.090722, reg: 0.023582)\n",
      "Iteration 100, Loss: 1.078959 (data: 1.055608, reg: 0.023352)\n",
      "Iteration 200, Loss: 1.049739 (data: 1.026329, reg: 0.023410)\n",
      "Iteration 300, Loss: 1.024909 (data: 1.001220, reg: 0.023689)\n",
      "Iteration 400, Loss: 1.003331 (data: 0.979192, reg: 0.024139)\n",
      "Iteration 500, Loss: 0.984257 (data: 0.959529, reg: 0.024729)\n",
      "Iteration 600, Loss: 0.967186 (data: 0.941753, reg: 0.025433)\n",
      "Iteration 700, Loss: 0.951767 (data: 0.925534, reg: 0.026233)\n",
      "Iteration 800, Loss: 0.937744 (data: 0.910630, reg: 0.027114)\n",
      "Iteration 900, Loss: 0.924925 (data: 0.896862, reg: 0.028063)\n",
      "Iteration 1000, Loss: 0.913156 (data: 0.884086, reg: 0.029070)\n",
      "Iteration 1100, Loss: 0.902315 (data: 0.872189, reg: 0.030126)\n",
      "Iteration 1200, Loss: 0.892297 (data: 0.861074, reg: 0.031223)\n",
      "Iteration 1300, Loss: 0.883017 (data: 0.850662, reg: 0.032355)\n",
      "Iteration 1400, Loss: 0.874399 (data: 0.840884, reg: 0.033514)\n",
      "Iteration 1500, Loss: 0.866378 (data: 0.831680, reg: 0.034698)\n",
      "Iteration 1600, Loss: 0.858899 (data: 0.822999, reg: 0.035899)\n",
      "Iteration 1700, Loss: 0.851911 (data: 0.814795, reg: 0.037116)\n",
      "Iteration 1800, Loss: 0.845371 (data: 0.807028, reg: 0.038344)\n",
      "Iteration 1900, Loss: 0.839241 (data: 0.799661, reg: 0.039580)\n",
      "LR: 0.001, Iter: 2000, C: 10.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.108159 (data: 1.105820, reg: 0.002339)\n",
      "Iteration 100, Loss: 1.070032 (data: 1.067687, reg: 0.002345)\n",
      "Iteration 200, Loss: 1.038396 (data: 1.036012, reg: 0.002384)\n",
      "Iteration 300, Loss: 1.011375 (data: 1.008927, reg: 0.002447)\n",
      "Iteration 400, Loss: 0.987715 (data: 0.985185, reg: 0.002531)\n",
      "Iteration 500, Loss: 0.966596 (data: 0.963966, reg: 0.002630)\n",
      "Iteration 600, Loss: 0.947475 (data: 0.944732, reg: 0.002743)\n",
      "Iteration 700, Loss: 0.929981 (data: 0.927113, reg: 0.002868)\n",
      "Iteration 800, Loss: 0.913852 (data: 0.910850, reg: 0.003003)\n",
      "Iteration 900, Loss: 0.898896 (data: 0.895750, reg: 0.003147)\n",
      "Iteration 1000, Loss: 0.884964 (data: 0.881666, reg: 0.003299)\n",
      "Iteration 1100, Loss: 0.871938 (data: 0.868480, reg: 0.003458)\n",
      "Iteration 1200, Loss: 0.859719 (data: 0.856095, reg: 0.003624)\n",
      "Iteration 1300, Loss: 0.848226 (data: 0.844431, reg: 0.003795)\n",
      "Iteration 1400, Loss: 0.837389 (data: 0.833417, reg: 0.003972)\n",
      "Iteration 1500, Loss: 0.827148 (data: 0.822995, reg: 0.004153)\n",
      "Iteration 1600, Loss: 0.817449 (data: 0.813111, reg: 0.004338)\n",
      "Iteration 1700, Loss: 0.808247 (data: 0.803720, reg: 0.004527)\n",
      "Iteration 1800, Loss: 0.799500 (data: 0.794781, reg: 0.004720)\n",
      "Iteration 1900, Loss: 0.791172 (data: 0.786257, reg: 0.004915)\n",
      "LR: 0.001, Iter: 2000, C: 100.0, Val Acc: 0.6510\n",
      "Iteration 0, Loss: 235.464402 (data: 1.099164, reg: 234.365238)\n",
      "Iteration 100, Loss: 605216102728800526465651975308594843618461218165297653060323543319320368851679480219331328979607091769189797190213826510848.000000 (data: 18.928837, reg: 605216102728800526465651975308594843618461218165297653060323543319320368851679480219331328979607091769189797190213826510848.000000)\n",
      "Iteration 200, Loss: 1562819207487679104477477532833429634140581365011265512172677068967974413905914825298843622299985017878074553176614893414578687654820929046590349064428715691872393330665211869723715790418316366460960223629502110196930197984816714892728072142848.000000 (data: 18.928837, reg: 1562819207487679104477477532833429634140581365011265512172677068967974413905914825298843622299985017878074553176614893414578687654820929046590349064428715691872393330665211869723715790418316366460960223629502110196930197984816714892728072142848.000000)\n",
      "Iteration 300, Loss: inf (data: 18.928837, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 18.928837, reg: inf)\n",
      "LR: 0.005, Iter: 500, C: 0.001, Val Acc: 0.3356\n",
      "Iteration 0, Loss: 24.212065 (data: 1.103234, reg: 23.108832)\n",
      "Iteration 100, Loss: 1.095661 (data: 1.093773, reg: 0.001888)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/jtg758yd6_n9rfwplmbsj_xh0000gn/T/ipykernel_52940/3521502296.py:52: RuntimeWarning: overflow encountered in multiply\n",
      "  reg_loss = (lambda_reg / 2) * np.sum(W * W)  # L2 regularization term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200, Loss: 1.094945 (data: 1.093097, reg: 0.001848)\n",
      "Iteration 300, Loss: 1.094432 (data: 1.092616, reg: 0.001816)\n",
      "Iteration 400, Loss: 1.094065 (data: 1.092275, reg: 0.001790)\n",
      "LR: 0.005, Iter: 500, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.403429 (data: 1.091719, reg: 2.311710)\n",
      "Iteration 100, Loss: 1.080138 (data: 1.063629, reg: 0.016509)\n",
      "Iteration 200, Loss: 1.079739 (data: 1.063390, reg: 0.016349)\n",
      "Iteration 300, Loss: 1.079500 (data: 1.063309, reg: 0.016190)\n",
      "Iteration 400, Loss: 1.079319 (data: 1.063262, reg: 0.016057)\n",
      "LR: 0.005, Iter: 500, C: 0.1, Val Acc: 0.4094\n",
      "Iteration 0, Loss: 1.327112 (data: 1.097410, reg: 0.229702)\n",
      "Iteration 100, Loss: 1.091461 (data: 0.986445, reg: 0.105016)\n",
      "Iteration 200, Loss: 1.018115 (data: 0.942383, reg: 0.075732)\n",
      "Iteration 300, Loss: 0.993346 (data: 0.921563, reg: 0.071784)\n",
      "Iteration 400, Loss: 0.984707 (data: 0.911085, reg: 0.073622)\n",
      "LR: 0.005, Iter: 500, C: 1.0, Val Acc: 0.6309\n",
      "Iteration 0, Loss: 1.114815 (data: 1.091644, reg: 0.023171)\n",
      "Iteration 100, Loss: 0.984577 (data: 0.960268, reg: 0.024310)\n",
      "Iteration 200, Loss: 0.913065 (data: 0.884383, reg: 0.028682)\n",
      "Iteration 300, Loss: 0.866056 (data: 0.831694, reg: 0.034362)\n",
      "Iteration 400, Loss: 0.833067 (data: 0.792523, reg: 0.040543)\n",
      "LR: 0.005, Iter: 500, C: 10.0, Val Acc: 0.6846\n",
      "Iteration 0, Loss: 1.104910 (data: 1.102641, reg: 0.002269)\n",
      "Iteration 100, Loss: 0.967953 (data: 0.965397, reg: 0.002555)\n",
      "Iteration 200, Loss: 0.886247 (data: 0.883031, reg: 0.003215)\n",
      "Iteration 300, Loss: 0.828170 (data: 0.824106, reg: 0.004065)\n",
      "Iteration 400, Loss: 0.784037 (data: 0.779016, reg: 0.005022)\n",
      "LR: 0.005, Iter: 500, C: 100.0, Val Acc: 0.6913\n",
      "Iteration 0, Loss: 228.016845 (data: 1.102114, reg: 226.914731)\n",
      "Iteration 100, Loss: 585982548855070799674828062714916961065178764877806400455577611644026250647816710197407700866200764766545836196194863284224.000000 (data: 18.992357, reg: 585982548855070799674828062714916961065178764877806400455577611644026250647816710197407700866200764766545836196194863284224.000000)\n",
      "Iteration 200, Loss: 1513153365342062529025662161564644129959488808494631433061444868436164522559511690846864829979702207571688864882828423603468033336885922813341949409942113302620637930417982483375789587748428343387076621999701866655646899307159683279911045300224.000000 (data: 18.992357, reg: 1513153365342062529025662161564644129959488808494631433061444868436164522559511690846864829979702207571688864882828423603468033336885922813341949409942113302620637930417982483375789587748428343387076621999701866655646899307159683279911045300224.000000)\n",
      "Iteration 300, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 500, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.005, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.735297 (data: 1.097158, reg: 22.638139)\n",
      "Iteration 100, Loss: 1.095661 (data: 1.093773, reg: 0.001888)\n",
      "Iteration 200, Loss: 1.094945 (data: 1.093097, reg: 0.001848)\n",
      "Iteration 300, Loss: 1.094432 (data: 1.092616, reg: 0.001816)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/jtg758yd6_n9rfwplmbsj_xh0000gn/T/ipykernel_52940/3521502296.py:57: RuntimeWarning: overflow encountered in multiply\n",
      "  dW = (X.T @ (y_pred - y_one_hot)) / n_samples + lambda_reg * W\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 400, Loss: 1.094065 (data: 1.092275, reg: 0.001790)\n",
      "Iteration 500, Loss: 1.093803 (data: 1.092034, reg: 0.001769)\n",
      "Iteration 600, Loss: 1.093615 (data: 1.091864, reg: 0.001752)\n",
      "Iteration 700, Loss: 1.093482 (data: 1.091744, reg: 0.001738)\n",
      "Iteration 800, Loss: 1.093387 (data: 1.091661, reg: 0.001726)\n",
      "Iteration 900, Loss: 1.093319 (data: 1.091603, reg: 0.001717)\n",
      "LR: 0.005, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.373858 (data: 1.108798, reg: 2.265060)\n",
      "Iteration 100, Loss: 1.080068 (data: 1.063637, reg: 0.016432)\n",
      "Iteration 200, Loss: 1.079688 (data: 1.063372, reg: 0.016316)\n",
      "Iteration 300, Loss: 1.079461 (data: 1.063298, reg: 0.016163)\n",
      "Iteration 400, Loss: 1.079290 (data: 1.063256, reg: 0.016034)\n",
      "Iteration 500, Loss: 1.079161 (data: 1.063236, reg: 0.015925)\n",
      "Iteration 600, Loss: 1.079064 (data: 1.063231, reg: 0.015834)\n",
      "Iteration 700, Loss: 1.078992 (data: 1.063236, reg: 0.015756)\n",
      "Iteration 800, Loss: 1.078937 (data: 1.063247, reg: 0.015690)\n",
      "Iteration 900, Loss: 1.078896 (data: 1.063262, reg: 0.015634)\n",
      "LR: 0.005, Iter: 1000, C: 0.1, Val Acc: 0.3826\n",
      "Iteration 0, Loss: 1.342468 (data: 1.098060, reg: 0.244408)\n",
      "Iteration 100, Loss: 1.096673 (data: 0.986275, reg: 0.110398)\n",
      "Iteration 200, Loss: 1.019839 (data: 0.941877, reg: 0.077961)\n",
      "Iteration 300, Loss: 0.993945 (data: 0.921123, reg: 0.072822)\n",
      "Iteration 400, Loss: 0.984923 (data: 0.910773, reg: 0.074150)\n",
      "Iteration 500, Loss: 0.981697 (data: 0.905415, reg: 0.076282)\n",
      "Iteration 600, Loss: 0.980505 (data: 0.902557, reg: 0.077948)\n",
      "Iteration 700, Loss: 0.980040 (data: 0.900984, reg: 0.079057)\n",
      "Iteration 800, Loss: 0.979840 (data: 0.900084, reg: 0.079756)\n",
      "Iteration 900, Loss: 0.979739 (data: 0.899543, reg: 0.080196)\n",
      "LR: 0.005, Iter: 1000, C: 1.0, Val Acc: 0.6443\n",
      "Iteration 0, Loss: 1.111237 (data: 1.087531, reg: 0.023706)\n",
      "Iteration 100, Loss: 0.983313 (data: 0.958414, reg: 0.024899)\n",
      "Iteration 200, Loss: 0.912283 (data: 0.883021, reg: 0.029262)\n",
      "Iteration 300, Loss: 0.865476 (data: 0.830553, reg: 0.034923)\n",
      "Iteration 400, Loss: 0.832588 (data: 0.791506, reg: 0.041081)\n",
      "Iteration 500, Loss: 0.808486 (data: 0.761160, reg: 0.047326)\n",
      "Iteration 600, Loss: 0.790248 (data: 0.736810, reg: 0.053438)\n",
      "Iteration 700, Loss: 0.776094 (data: 0.716788, reg: 0.059306)\n",
      "Iteration 800, Loss: 0.764881 (data: 0.700004, reg: 0.064877)\n",
      "Iteration 900, Loss: 0.755846 (data: 0.685719, reg: 0.070127)\n",
      "LR: 0.005, Iter: 1000, C: 10.0, Val Acc: 0.7181\n",
      "Iteration 0, Loss: 1.104085 (data: 1.101819, reg: 0.002267)\n",
      "Iteration 100, Loss: 0.967288 (data: 0.964732, reg: 0.002556)\n",
      "Iteration 200, Loss: 0.885465 (data: 0.882245, reg: 0.003220)\n",
      "Iteration 300, Loss: 0.827255 (data: 0.823181, reg: 0.004074)\n",
      "Iteration 400, Loss: 0.783025 (data: 0.777988, reg: 0.005037)\n",
      "Iteration 500, Loss: 0.747937 (data: 0.741874, reg: 0.006063)\n",
      "Iteration 600, Loss: 0.719181 (data: 0.712055, reg: 0.007126)\n",
      "Iteration 700, Loss: 0.695006 (data: 0.686797, reg: 0.008209)\n",
      "Iteration 800, Loss: 0.674260 (data: 0.664957, reg: 0.009303)\n",
      "Iteration 900, Loss: 0.656156 (data: 0.645755, reg: 0.010401)\n",
      "LR: 0.005, Iter: 1000, C: 100.0, Val Acc: 0.7181\n",
      "Iteration 0, Loss: 237.130882 (data: 1.101024, reg: 236.029859)\n",
      "Iteration 100, Loss: 609517510446516475010266232145501068651135598388545518574312731149307963613230060658422483115448942578579275164289282867200.000000 (data: 18.420681, reg: 609517510446516475010266232145501068651135598388545518574312731149307963613230060658422483115448942578579275164289282867200.000000)\n",
      "Iteration 200, Loss: 1573926517042353200323743754287052038239689027949896144290596504968118311467570056511951446818912534956568532522538049779220645502426241195613320857466263906413529936464379908873373143193732501808250899465429313780861233693822388809422287667200.000000 (data: 18.420681, reg: 1573926517042353200323743754287052038239689027949896144290596504968118311467570056511951446818912534956568532522538049779220645502426241195613320857466263906413529936464379908873373143193732501808250899465429313780861233693822388809422287667200.000000)\n",
      "Iteration 300, Loss: inf (data: 18.420681, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 18.420681, reg: inf)\n",
      "Iteration 500, Loss: inf (data: 18.420681, reg: inf)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.005, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.795890 (data: 1.092982, reg: 22.702908)\n",
      "Iteration 100, Loss: 1.095669 (data: 1.093780, reg: 0.001888)\n",
      "Iteration 200, Loss: 1.094951 (data: 1.093103, reg: 0.001848)\n",
      "Iteration 300, Loss: 1.094436 (data: 1.092620, reg: 0.001816)\n",
      "Iteration 400, Loss: 1.094068 (data: 1.092278, reg: 0.001790)\n",
      "Iteration 500, Loss: 1.093805 (data: 1.092036, reg: 0.001769)\n",
      "Iteration 600, Loss: 1.093617 (data: 1.091865, reg: 0.001752)\n",
      "Iteration 700, Loss: 1.093483 (data: 1.091745, reg: 0.001738)\n",
      "Iteration 800, Loss: 1.093388 (data: 1.091662, reg: 0.001726)\n",
      "Iteration 900, Loss: 1.093320 (data: 1.091603, reg: 0.001717)\n",
      "Iteration 1000, Loss: 1.093272 (data: 1.091563, reg: 0.001709)\n",
      "Iteration 1100, Loss: 1.093238 (data: 1.091535, reg: 0.001703)\n",
      "Iteration 1200, Loss: 1.093213 (data: 1.091516, reg: 0.001697)\n",
      "Iteration 1300, Loss: 1.093196 (data: 1.091503, reg: 0.001693)\n",
      "Iteration 1400, Loss: 1.093184 (data: 1.091494, reg: 0.001689)\n",
      "Iteration 1500, Loss: 1.093175 (data: 1.091489, reg: 0.001686)\n",
      "Iteration 1600, Loss: 1.093169 (data: 1.091485, reg: 0.001684)\n",
      "Iteration 1700, Loss: 1.093164 (data: 1.091483, reg: 0.001681)\n",
      "Iteration 1800, Loss: 1.093161 (data: 1.091482, reg: 0.001680)\n",
      "Iteration 1900, Loss: 1.093159 (data: 1.091481, reg: 0.001678)\n",
      "LR: 0.005, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.417903 (data: 1.102369, reg: 2.315534)\n",
      "Iteration 100, Loss: 1.080091 (data: 1.063630, reg: 0.016461)\n",
      "Iteration 200, Loss: 1.079704 (data: 1.063377, reg: 0.016326)\n",
      "Iteration 300, Loss: 1.079473 (data: 1.063301, reg: 0.016171)\n",
      "Iteration 400, Loss: 1.079299 (data: 1.063258, reg: 0.016041)\n",
      "Iteration 500, Loss: 1.079168 (data: 1.063237, reg: 0.015931)\n",
      "Iteration 600, Loss: 1.079070 (data: 1.063231, reg: 0.015838)\n",
      "Iteration 700, Loss: 1.078995 (data: 1.063236, reg: 0.015760)\n",
      "Iteration 800, Loss: 1.078940 (data: 1.063246, reg: 0.015693)\n",
      "Iteration 900, Loss: 1.078898 (data: 1.063261, reg: 0.015637)\n",
      "Iteration 1000, Loss: 1.078867 (data: 1.063278, reg: 0.015589)\n",
      "Iteration 1100, Loss: 1.078843 (data: 1.063296, reg: 0.015547)\n",
      "Iteration 1200, Loss: 1.078825 (data: 1.063313, reg: 0.015512)\n",
      "Iteration 1300, Loss: 1.078812 (data: 1.063330, reg: 0.015482)\n",
      "Iteration 1400, Loss: 1.078802 (data: 1.063346, reg: 0.015456)\n",
      "Iteration 1500, Loss: 1.078795 (data: 1.063361, reg: 0.015434)\n",
      "Iteration 1600, Loss: 1.078789 (data: 1.063374, reg: 0.015415)\n",
      "Iteration 1700, Loss: 1.078785 (data: 1.063386, reg: 0.015399)\n",
      "Iteration 1800, Loss: 1.078782 (data: 1.063397, reg: 0.015384)\n",
      "Iteration 1900, Loss: 1.078779 (data: 1.063407, reg: 0.015372)\n",
      "LR: 0.005, Iter: 2000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.325224 (data: 1.094463, reg: 0.230762)\n",
      "Iteration 100, Loss: 1.089838 (data: 0.983054, reg: 0.106784)\n",
      "Iteration 200, Loss: 1.017568 (data: 0.940329, reg: 0.077238)\n",
      "Iteration 300, Loss: 0.993193 (data: 0.920381, reg: 0.072812)\n",
      "Iteration 400, Loss: 0.984687 (data: 0.910426, reg: 0.074260)\n",
      "Iteration 500, Loss: 0.981637 (data: 0.905268, reg: 0.076370)\n",
      "Iteration 600, Loss: 0.980506 (data: 0.902512, reg: 0.077994)\n",
      "Iteration 700, Loss: 0.980060 (data: 0.900992, reg: 0.079068)\n",
      "Iteration 800, Loss: 0.979865 (data: 0.900120, reg: 0.079745)\n",
      "Iteration 900, Loss: 0.979764 (data: 0.899592, reg: 0.080172)\n",
      "Iteration 1000, Loss: 0.979700 (data: 0.899251, reg: 0.080448)\n",
      "Iteration 1100, Loss: 0.979652 (data: 0.899014, reg: 0.080638)\n",
      "Iteration 1200, Loss: 0.979613 (data: 0.898837, reg: 0.080776)\n",
      "Iteration 1300, Loss: 0.979580 (data: 0.898695, reg: 0.080885)\n",
      "Iteration 1400, Loss: 0.979551 (data: 0.898575, reg: 0.080975)\n",
      "Iteration 1500, Loss: 0.979525 (data: 0.898470, reg: 0.081054)\n",
      "Iteration 1600, Loss: 0.979502 (data: 0.898376, reg: 0.081126)\n",
      "Iteration 1700, Loss: 0.979482 (data: 0.898290, reg: 0.081191)\n",
      "Iteration 1800, Loss: 0.979463 (data: 0.898211, reg: 0.081253)\n",
      "Iteration 1900, Loss: 0.979447 (data: 0.898137, reg: 0.081310)\n",
      "LR: 0.005, Iter: 2000, C: 1.0, Val Acc: 0.6577\n",
      "Iteration 0, Loss: 1.125422 (data: 1.102265, reg: 0.023157)\n",
      "Iteration 100, Loss: 0.989452 (data: 0.965429, reg: 0.024023)\n",
      "Iteration 200, Loss: 0.916206 (data: 0.887913, reg: 0.028293)\n",
      "Iteration 300, Loss: 0.868334 (data: 0.834415, reg: 0.033919)\n",
      "Iteration 400, Loss: 0.834819 (data: 0.794750, reg: 0.040069)\n",
      "Iteration 500, Loss: 0.810305 (data: 0.763988, reg: 0.046317)\n",
      "Iteration 600, Loss: 0.791774 (data: 0.739333, reg: 0.052441)\n",
      "Iteration 700, Loss: 0.777397 (data: 0.719071, reg: 0.058326)\n",
      "Iteration 800, Loss: 0.766006 (data: 0.702089, reg: 0.063917)\n",
      "Iteration 900, Loss: 0.756825 (data: 0.687635, reg: 0.069190)\n",
      "Iteration 1000, Loss: 0.749319 (data: 0.675175, reg: 0.074144)\n",
      "Iteration 1100, Loss: 0.743108 (data: 0.664325, reg: 0.078783)\n",
      "Iteration 1200, Loss: 0.737917 (data: 0.654794, reg: 0.083122)\n",
      "Iteration 1300, Loss: 0.733540 (data: 0.646364, reg: 0.087176)\n",
      "Iteration 1400, Loss: 0.729823 (data: 0.638862, reg: 0.090961)\n",
      "Iteration 1500, Loss: 0.726645 (data: 0.632151, reg: 0.094495)\n",
      "Iteration 1600, Loss: 0.723914 (data: 0.626121, reg: 0.097793)\n",
      "Iteration 1700, Loss: 0.721553 (data: 0.620682, reg: 0.100871)\n",
      "Iteration 1800, Loss: 0.719505 (data: 0.615759, reg: 0.103746)\n",
      "Iteration 1900, Loss: 0.717720 (data: 0.611290, reg: 0.106430)\n",
      "LR: 0.005, Iter: 2000, C: 10.0, Val Acc: 0.7248\n",
      "Iteration 0, Loss: 1.100798 (data: 1.098491, reg: 0.002308)\n",
      "Iteration 100, Loss: 0.965674 (data: 0.963068, reg: 0.002605)\n",
      "Iteration 200, Loss: 0.884855 (data: 0.881586, reg: 0.003269)\n",
      "Iteration 300, Loss: 0.827274 (data: 0.823156, reg: 0.004118)\n",
      "Iteration 400, Loss: 0.783434 (data: 0.778361, reg: 0.005074)\n",
      "Iteration 500, Loss: 0.748581 (data: 0.742489, reg: 0.006092)\n",
      "Iteration 600, Loss: 0.719955 (data: 0.712809, reg: 0.007146)\n",
      "Iteration 700, Loss: 0.695839 (data: 0.687619, reg: 0.008220)\n",
      "Iteration 800, Loss: 0.675107 (data: 0.665801, reg: 0.009307)\n",
      "Iteration 900, Loss: 0.656988 (data: 0.646590, reg: 0.010398)\n",
      "Iteration 1000, Loss: 0.640937 (data: 0.629447, reg: 0.011490)\n",
      "Iteration 1100, Loss: 0.626559 (data: 0.613977, reg: 0.012582)\n",
      "Iteration 1200, Loss: 0.613557 (data: 0.599887, reg: 0.013671)\n",
      "Iteration 1300, Loss: 0.601708 (data: 0.586953, reg: 0.014755)\n",
      "Iteration 1400, Loss: 0.590837 (data: 0.575002, reg: 0.015836)\n",
      "Iteration 1500, Loss: 0.580807 (data: 0.563896, reg: 0.016911)\n",
      "Iteration 1600, Loss: 0.571507 (data: 0.553526, reg: 0.017981)\n",
      "Iteration 1700, Loss: 0.562847 (data: 0.543802, reg: 0.019045)\n",
      "Iteration 1800, Loss: 0.554754 (data: 0.534650, reg: 0.020103)\n",
      "Iteration 1900, Loss: 0.547166 (data: 0.526010, reg: 0.021156)\n",
      "LR: 0.005, Iter: 2000, C: 100.0, Val Acc: 0.7383\n",
      "Iteration 0, Loss: 230.764083 (data: 1.109680, reg: 229.654402)\n",
      "Iteration 100, Loss: 16203544868069227452730483059097771593810798554744367036128599377587306603362118677364505817400485055443890245938831478971628864329230615529974791600164764859565310258173406414636767062911877120.000000 (data: 21.152023, reg: 16203544868069227452730483059097771593810798554744367036128599377587306603362118677364505817400485055443890245938831478971628864329230615529974791600164764859565310258173406414636767062911877120.000000)\n",
      "Iteration 200, Loss: inf (data: 21.152023, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 21.152023, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.01, Iter: 500, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 24.713710 (data: 1.098993, reg: 23.614717)\n",
      "Iteration 100, Loss: 1.094947 (data: 1.093098, reg: 0.001848)\n",
      "Iteration 200, Loss: 1.094065 (data: 1.092275, reg: 0.001790)\n",
      "Iteration 300, Loss: 1.093615 (data: 1.091864, reg: 0.001752)\n",
      "Iteration 400, Loss: 1.093387 (data: 1.091661, reg: 0.001726)\n",
      "LR: 0.01, Iter: 500, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.378233 (data: 1.097563, reg: 2.280669)\n",
      "Iteration 100, Loss: 1.079700 (data: 1.063376, reg: 0.016324)\n",
      "Iteration 200, Loss: 1.079297 (data: 1.063257, reg: 0.016039)\n",
      "Iteration 300, Loss: 1.079068 (data: 1.063231, reg: 0.015837)\n",
      "Iteration 400, Loss: 1.078939 (data: 1.063247, reg: 0.015692)\n",
      "LR: 0.01, Iter: 500, C: 0.1, Val Acc: 0.3826\n",
      "Iteration 0, Loss: 1.333559 (data: 1.101706, reg: 0.231853)\n",
      "Iteration 100, Loss: 1.017990 (data: 0.941804, reg: 0.076186)\n",
      "Iteration 200, Loss: 0.984668 (data: 0.910748, reg: 0.073920)\n",
      "Iteration 300, Loss: 0.980478 (data: 0.902566, reg: 0.077912)\n",
      "Iteration 400, Loss: 0.979844 (data: 0.900103, reg: 0.079741)\n",
      "LR: 0.01, Iter: 500, C: 1.0, Val Acc: 0.6510\n",
      "Iteration 0, Loss: 1.117544 (data: 1.095382, reg: 0.022162)\n",
      "Iteration 100, Loss: 0.913643 (data: 0.886010, reg: 0.027633)\n",
      "Iteration 200, Loss: 0.832347 (data: 0.792594, reg: 0.039753)\n",
      "Iteration 300, Loss: 0.789666 (data: 0.737256, reg: 0.052410)\n",
      "Iteration 400, Loss: 0.764290 (data: 0.700201, reg: 0.064089)\n",
      "LR: 0.01, Iter: 500, C: 10.0, Val Acc: 0.6980\n",
      "Iteration 0, Loss: 1.102260 (data: 1.099966, reg: 0.002294)\n",
      "Iteration 100, Loss: 0.883519 (data: 0.880245, reg: 0.003274)\n",
      "Iteration 200, Loss: 0.782438 (data: 0.777348, reg: 0.005091)\n",
      "Iteration 300, Loss: 0.719116 (data: 0.711944, reg: 0.007172)\n",
      "Iteration 400, Loss: 0.674396 (data: 0.665055, reg: 0.009342)\n",
      "LR: 0.01, Iter: 500, C: 100.0, Val Acc: 0.7181\n",
      "Iteration 0, Loss: 224.105929 (data: 1.107081, reg: 222.998848)\n",
      "Iteration 100, Loss: 15733826230010320519885468036665971937338645875612591275190492161969878317592547225232502595656934341617973283448638483708850005995797512337835377232131029916722742372163802189991335909300109312.000000 (data: 19.436994, reg: 15733826230010320519885468036665971937338645875612591275190492161969878317592547225232502595656934341617973283448638483708850005995797512337835377232131029916722742372163802189991335909300109312.000000)\n",
      "Iteration 200, Loss: inf (data: 19.436994, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 19.436994, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.01, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.744961 (data: 1.100880, reg: 22.644081)\n",
      "Iteration 100, Loss: 1.094945 (data: 1.093097, reg: 0.001848)\n",
      "Iteration 200, Loss: 1.094064 (data: 1.092275, reg: 0.001790)\n",
      "Iteration 300, Loss: 1.093615 (data: 1.091863, reg: 0.001752)\n",
      "Iteration 400, Loss: 1.093387 (data: 1.091661, reg: 0.001726)\n",
      "Iteration 500, Loss: 1.093271 (data: 1.091562, reg: 0.001709)\n",
      "Iteration 600, Loss: 1.093213 (data: 1.091516, reg: 0.001697)\n",
      "Iteration 700, Loss: 1.093183 (data: 1.091494, reg: 0.001689)\n",
      "Iteration 800, Loss: 1.093169 (data: 1.091485, reg: 0.001684)\n",
      "Iteration 900, Loss: 1.093161 (data: 1.091482, reg: 0.001680)\n",
      "LR: 0.01, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.366072 (data: 1.113993, reg: 2.252079)\n",
      "Iteration 100, Loss: 1.079700 (data: 1.063376, reg: 0.016324)\n",
      "Iteration 200, Loss: 1.079297 (data: 1.063257, reg: 0.016039)\n",
      "Iteration 300, Loss: 1.079068 (data: 1.063231, reg: 0.015837)\n",
      "Iteration 400, Loss: 1.078939 (data: 1.063247, reg: 0.015692)\n",
      "Iteration 500, Loss: 1.078866 (data: 1.063278, reg: 0.015588)\n",
      "Iteration 600, Loss: 1.078825 (data: 1.063313, reg: 0.015512)\n",
      "Iteration 700, Loss: 1.078802 (data: 1.063346, reg: 0.015456)\n",
      "Iteration 800, Loss: 1.078789 (data: 1.063374, reg: 0.015415)\n",
      "Iteration 900, Loss: 1.078782 (data: 1.063397, reg: 0.015384)\n",
      "LR: 0.01, Iter: 1000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.335445 (data: 1.102630, reg: 0.232815)\n",
      "Iteration 100, Loss: 1.017985 (data: 0.941325, reg: 0.076660)\n",
      "Iteration 200, Loss: 0.984697 (data: 0.910625, reg: 0.074072)\n",
      "Iteration 300, Loss: 0.980505 (data: 0.902557, reg: 0.077948)\n",
      "Iteration 400, Loss: 0.979866 (data: 0.900133, reg: 0.079733)\n",
      "Iteration 500, Loss: 0.979701 (data: 0.899257, reg: 0.080444)\n",
      "Iteration 600, Loss: 0.979614 (data: 0.898841, reg: 0.080773)\n",
      "Iteration 700, Loss: 0.979552 (data: 0.898579, reg: 0.080973)\n",
      "Iteration 800, Loss: 0.979503 (data: 0.898379, reg: 0.081123)\n",
      "Iteration 900, Loss: 0.979464 (data: 0.898213, reg: 0.081251)\n",
      "LR: 0.01, Iter: 1000, C: 1.0, Val Acc: 0.6577\n",
      "Iteration 0, Loss: 1.130510 (data: 1.108221, reg: 0.022289)\n",
      "Iteration 100, Loss: 0.918924 (data: 0.891741, reg: 0.027183)\n",
      "Iteration 200, Loss: 0.836212 (data: 0.797262, reg: 0.038950)\n",
      "Iteration 300, Loss: 0.792423 (data: 0.740996, reg: 0.051426)\n",
      "Iteration 400, Loss: 0.766262 (data: 0.703221, reg: 0.063041)\n",
      "Iteration 500, Loss: 0.749375 (data: 0.675967, reg: 0.073408)\n",
      "Iteration 600, Loss: 0.737876 (data: 0.655362, reg: 0.082515)\n",
      "Iteration 700, Loss: 0.729741 (data: 0.639277, reg: 0.090463)\n",
      "Iteration 800, Loss: 0.723818 (data: 0.626431, reg: 0.097387)\n",
      "Iteration 900, Loss: 0.719410 (data: 0.615994, reg: 0.103416)\n",
      "LR: 0.01, Iter: 1000, C: 10.0, Val Acc: 0.7181\n",
      "Iteration 0, Loss: 1.099704 (data: 1.097415, reg: 0.002289)\n",
      "Iteration 100, Loss: 0.882375 (data: 0.879097, reg: 0.003277)\n",
      "Iteration 200, Loss: 0.781148 (data: 0.776039, reg: 0.005109)\n",
      "Iteration 300, Loss: 0.717968 (data: 0.710765, reg: 0.007202)\n",
      "Iteration 400, Loss: 0.673410 (data: 0.664029, reg: 0.009381)\n",
      "Iteration 500, Loss: 0.639496 (data: 0.627919, reg: 0.011578)\n",
      "Iteration 600, Loss: 0.612337 (data: 0.598570, reg: 0.013768)\n",
      "Iteration 700, Loss: 0.589804 (data: 0.573865, reg: 0.015940)\n",
      "Iteration 800, Loss: 0.570631 (data: 0.552541, reg: 0.018089)\n",
      "Iteration 900, Loss: 0.554010 (data: 0.533795, reg: 0.020215)\n",
      "LR: 0.01, Iter: 1000, C: 100.0, Val Acc: 0.7383\n",
      "Iteration 0, Loss: 232.249955 (data: 1.104739, reg: 231.145217)\n",
      "Iteration 100, Loss: 16308333594060471553575186311725485045443504636034632243830958413712485761541851846618723421571885187777987924303438565680069548305062433410081816651055824225304815372580886505990207212615630848.000000 (data: 18.992357, reg: 16308333594060471553575186311725485045443504636034632243830958413712485761541851846618723421571885187777987924303438565680069548305062433410081816651055824225304815372580886505990207212615630848.000000)\n",
      "Iteration 200, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.01, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.985600 (data: 1.102492, reg: 22.883108)\n",
      "Iteration 100, Loss: 1.094944 (data: 1.093096, reg: 0.001848)\n",
      "Iteration 200, Loss: 1.094064 (data: 1.092274, reg: 0.001790)\n",
      "Iteration 300, Loss: 1.093615 (data: 1.091863, reg: 0.001752)\n",
      "Iteration 400, Loss: 1.093387 (data: 1.091660, reg: 0.001726)\n",
      "Iteration 500, Loss: 1.093271 (data: 1.091562, reg: 0.001709)\n",
      "Iteration 600, Loss: 1.093213 (data: 1.091516, reg: 0.001697)\n",
      "Iteration 700, Loss: 1.093183 (data: 1.091494, reg: 0.001689)\n",
      "Iteration 800, Loss: 1.093169 (data: 1.091485, reg: 0.001684)\n",
      "Iteration 900, Loss: 1.093161 (data: 1.091482, reg: 0.001680)\n",
      "Iteration 1000, Loss: 1.093157 (data: 1.091481, reg: 0.001677)\n",
      "Iteration 1100, Loss: 1.093156 (data: 1.091481, reg: 0.001675)\n",
      "Iteration 1200, Loss: 1.093155 (data: 1.091481, reg: 0.001674)\n",
      "Iteration 1300, Loss: 1.093154 (data: 1.091482, reg: 0.001673)\n",
      "Iteration 1400, Loss: 1.093154 (data: 1.091482, reg: 0.001672)\n",
      "Iteration 1500, Loss: 1.093154 (data: 1.091482, reg: 0.001671)\n",
      "Iteration 1600, Loss: 1.093154 (data: 1.091483, reg: 0.001671)\n",
      "Iteration 1700, Loss: 1.093154 (data: 1.091483, reg: 0.001671)\n",
      "Iteration 1800, Loss: 1.093154 (data: 1.091483, reg: 0.001671)\n",
      "Iteration 1900, Loss: 1.093154 (data: 1.091483, reg: 0.001671)\n",
      "LR: 0.01, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.336942 (data: 1.093831, reg: 2.243112)\n",
      "Iteration 100, Loss: 1.079707 (data: 1.063378, reg: 0.016329)\n",
      "Iteration 200, Loss: 1.079300 (data: 1.063258, reg: 0.016042)\n",
      "Iteration 300, Loss: 1.079070 (data: 1.063231, reg: 0.015839)\n",
      "Iteration 400, Loss: 1.078940 (data: 1.063246, reg: 0.015694)\n",
      "Iteration 500, Loss: 1.078867 (data: 1.063278, reg: 0.015589)\n",
      "Iteration 600, Loss: 1.078825 (data: 1.063313, reg: 0.015512)\n",
      "Iteration 700, Loss: 1.078802 (data: 1.063346, reg: 0.015456)\n",
      "Iteration 800, Loss: 1.078789 (data: 1.063374, reg: 0.015415)\n",
      "Iteration 900, Loss: 1.078782 (data: 1.063397, reg: 0.015384)\n",
      "Iteration 1000, Loss: 1.078777 (data: 1.063416, reg: 0.015362)\n",
      "Iteration 1100, Loss: 1.078775 (data: 1.063430, reg: 0.015345)\n",
      "Iteration 1200, Loss: 1.078774 (data: 1.063441, reg: 0.015332)\n",
      "Iteration 1300, Loss: 1.078773 (data: 1.063450, reg: 0.015323)\n",
      "Iteration 1400, Loss: 1.078773 (data: 1.063457, reg: 0.015316)\n",
      "Iteration 1500, Loss: 1.078772 (data: 1.063462, reg: 0.015311)\n",
      "Iteration 1600, Loss: 1.078772 (data: 1.063466, reg: 0.015307)\n",
      "Iteration 1700, Loss: 1.078772 (data: 1.063468, reg: 0.015304)\n",
      "Iteration 1800, Loss: 1.078772 (data: 1.063471, reg: 0.015302)\n",
      "Iteration 1900, Loss: 1.078772 (data: 1.063472, reg: 0.015300)\n",
      "LR: 0.01, Iter: 2000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.340053 (data: 1.110782, reg: 0.229271)\n",
      "Iteration 100, Loss: 1.018271 (data: 0.943476, reg: 0.074796)\n",
      "Iteration 200, Loss: 0.984734 (data: 0.911350, reg: 0.073383)\n",
      "Iteration 300, Loss: 0.980519 (data: 0.902820, reg: 0.077700)\n",
      "Iteration 400, Loss: 0.979877 (data: 0.900239, reg: 0.079638)\n",
      "Iteration 500, Loss: 0.979709 (data: 0.899308, reg: 0.080401)\n",
      "Iteration 600, Loss: 0.979621 (data: 0.898872, reg: 0.080749)\n",
      "Iteration 700, Loss: 0.979557 (data: 0.898601, reg: 0.080956)\n",
      "Iteration 800, Loss: 0.979507 (data: 0.898397, reg: 0.081110)\n",
      "Iteration 900, Loss: 0.979467 (data: 0.898228, reg: 0.081239)\n",
      "Iteration 1000, Loss: 0.979436 (data: 0.898083, reg: 0.081353)\n",
      "Iteration 1100, Loss: 0.979411 (data: 0.897957, reg: 0.081454)\n",
      "Iteration 1200, Loss: 0.979391 (data: 0.897845, reg: 0.081546)\n",
      "Iteration 1300, Loss: 0.979376 (data: 0.897747, reg: 0.081629)\n",
      "Iteration 1400, Loss: 0.979363 (data: 0.897660, reg: 0.081704)\n",
      "Iteration 1500, Loss: 0.979354 (data: 0.897583, reg: 0.081771)\n",
      "Iteration 1600, Loss: 0.979346 (data: 0.897515, reg: 0.081831)\n",
      "Iteration 1700, Loss: 0.979340 (data: 0.897455, reg: 0.081885)\n",
      "Iteration 1800, Loss: 0.979335 (data: 0.897401, reg: 0.081933)\n",
      "Iteration 1900, Loss: 0.979331 (data: 0.897354, reg: 0.081977)\n",
      "LR: 0.01, Iter: 2000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.124488 (data: 1.102790, reg: 0.021699)\n",
      "Iteration 100, Loss: 0.916559 (data: 0.889647, reg: 0.026912)\n",
      "Iteration 200, Loss: 0.834193 (data: 0.795264, reg: 0.038929)\n",
      "Iteration 300, Loss: 0.790810 (data: 0.739222, reg: 0.051588)\n",
      "Iteration 400, Loss: 0.765006 (data: 0.701684, reg: 0.063322)\n",
      "Iteration 500, Loss: 0.748407 (data: 0.674646, reg: 0.073761)\n",
      "Iteration 600, Loss: 0.737135 (data: 0.654230, reg: 0.082905)\n",
      "Iteration 700, Loss: 0.729174 (data: 0.638308, reg: 0.090866)\n",
      "Iteration 800, Loss: 0.723386 (data: 0.625600, reg: 0.097786)\n",
      "Iteration 900, Loss: 0.719082 (data: 0.615282, reg: 0.103801)\n",
      "Iteration 1000, Loss: 0.715824 (data: 0.606792, reg: 0.109032)\n",
      "Iteration 1100, Loss: 0.713319 (data: 0.599732, reg: 0.113587)\n",
      "Iteration 1200, Loss: 0.711368 (data: 0.593810, reg: 0.117558)\n",
      "Iteration 1300, Loss: 0.709832 (data: 0.588808, reg: 0.121024)\n",
      "Iteration 1400, Loss: 0.708608 (data: 0.584555, reg: 0.124053)\n",
      "Iteration 1500, Loss: 0.707623 (data: 0.580921, reg: 0.126702)\n",
      "Iteration 1600, Loss: 0.706821 (data: 0.577800, reg: 0.129022)\n",
      "Iteration 1700, Loss: 0.706163 (data: 0.575108, reg: 0.131055)\n",
      "Iteration 1800, Loss: 0.705617 (data: 0.572777, reg: 0.132840)\n",
      "Iteration 1900, Loss: 0.705159 (data: 0.570752, reg: 0.134407)\n",
      "LR: 0.01, Iter: 2000, C: 10.0, Val Acc: 0.7383\n",
      "Iteration 0, Loss: 1.105330 (data: 1.103004, reg: 0.002326)\n",
      "Iteration 100, Loss: 0.886099 (data: 0.882828, reg: 0.003271)\n",
      "Iteration 200, Loss: 0.783626 (data: 0.778543, reg: 0.005082)\n",
      "Iteration 300, Loss: 0.719905 (data: 0.712743, reg: 0.007162)\n",
      "Iteration 400, Loss: 0.675029 (data: 0.665701, reg: 0.009328)\n",
      "Iteration 500, Loss: 0.640891 (data: 0.629377, reg: 0.011515)\n",
      "Iteration 600, Loss: 0.613561 (data: 0.599865, reg: 0.013696)\n",
      "Iteration 700, Loss: 0.590891 (data: 0.575031, reg: 0.015860)\n",
      "Iteration 800, Loss: 0.571607 (data: 0.553603, reg: 0.018004)\n",
      "Iteration 900, Loss: 0.554895 (data: 0.534770, reg: 0.020125)\n",
      "Iteration 1000, Loss: 0.540207 (data: 0.517985, reg: 0.022222)\n",
      "Iteration 1100, Loss: 0.527158 (data: 0.502864, reg: 0.024294)\n",
      "Iteration 1200, Loss: 0.515466 (data: 0.489124, reg: 0.026341)\n",
      "Iteration 1300, Loss: 0.504915 (data: 0.476552, reg: 0.028363)\n",
      "Iteration 1400, Loss: 0.495341 (data: 0.464981, reg: 0.030360)\n",
      "Iteration 1500, Loss: 0.486611 (data: 0.454281, reg: 0.032330)\n",
      "Iteration 1600, Loss: 0.478618 (data: 0.444344, reg: 0.034274)\n",
      "Iteration 1700, Loss: 0.471274 (data: 0.435083, reg: 0.036191)\n",
      "Iteration 1800, Loss: 0.464505 (data: 0.426424, reg: 0.038082)\n",
      "Iteration 1900, Loss: 0.458250 (data: 0.418305, reg: 0.039944)\n",
      "LR: 0.01, Iter: 2000, C: 100.0, Val Acc: 0.7383\n",
      "Iteration 0, Loss: 234.262219 (data: 1.101346, reg: 233.160873)\n",
      "Iteration 100, Loss: inf (data: 19.818112, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 500, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.839304 (data: 1.091449, reg: 22.747855)\n",
      "Iteration 100, Loss: 58756953680109541710188001017958591151252097299060414921028062663838304319445561375162552848456685382930198068905056927744.000000 (data: 17.531407, reg: 58756953680109541710188001017958591151252097299060414921028062663838304319445561375162552848456685382930198068905056927744.000000)\n",
      "Iteration 200, Loss: 151725136477220999084585367028433305930900764450319956947763252974605994802054400619630152262334105585709783516141467266773615642085280711839014002382637838675051483325593286919133510318247963024182132959909672550327565505717331617559550623744.000000 (data: 17.531407, reg: 151725136477220999084585367028433305930900764450319956947763252974605994802054400619630152262334105585709783516141467266773615642085280711839014002382637838675051483325593286919133510318247963024182132959909672550327565505717331617559550623744.000000)\n",
      "Iteration 300, Loss: inf (data: 17.531407, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 17.531407, reg: inf)\n",
      "LR: 0.05, Iter: 500, C: 0.01, Val Acc: 0.3826\n",
      "Iteration 0, Loss: 3.409991 (data: 1.101839, reg: 2.308152)\n",
      "Iteration 100, Loss: 1.078865 (data: 1.063279, reg: 0.015587)\n",
      "Iteration 200, Loss: 1.078777 (data: 1.063417, reg: 0.015361)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063462, reg: 0.015310)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063474, reg: 0.015299)\n",
      "LR: 0.05, Iter: 500, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.334582 (data: 1.111166, reg: 0.223416)\n",
      "Iteration 100, Loss: 0.979717 (data: 0.899286, reg: 0.080431)\n",
      "Iteration 200, Loss: 0.979439 (data: 0.898096, reg: 0.081343)\n",
      "Iteration 300, Loss: 0.979354 (data: 0.897589, reg: 0.081765)\n",
      "Iteration 400, Loss: 0.979328 (data: 0.897315, reg: 0.082013)\n",
      "LR: 0.05, Iter: 500, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.110069 (data: 1.087261, reg: 0.022808)\n",
      "Iteration 100, Loss: 0.748470 (data: 0.673787, reg: 0.074682)\n",
      "Iteration 200, Loss: 0.715913 (data: 0.606590, reg: 0.109323)\n",
      "Iteration 300, Loss: 0.707632 (data: 0.580825, reg: 0.126806)\n",
      "Iteration 400, Loss: 0.704757 (data: 0.568923, reg: 0.135834)\n",
      "LR: 0.05, Iter: 500, C: 10.0, Val Acc: 0.7450\n",
      "Iteration 0, Loss: 1.102538 (data: 1.100229, reg: 0.002309)\n",
      "Iteration 100, Loss: 0.641260 (data: 0.629760, reg: 0.011500)\n",
      "Iteration 200, Loss: 0.540518 (data: 0.518325, reg: 0.022192)\n",
      "Iteration 300, Loss: 0.486850 (data: 0.454556, reg: 0.032294)\n",
      "Iteration 400, Loss: 0.452637 (data: 0.410897, reg: 0.041740)\n",
      "LR: 0.05, Iter: 500, C: 100.0, Val Acc: 0.7181\n",
      "Iteration 0, Loss: 224.521911 (data: 1.116373, reg: 223.405538)\n",
      "Iteration 100, Loss: inf (data: 21.024984, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 24.165317 (data: 1.098920, reg: 23.066397)\n",
      "Iteration 100, Loss: 59592921637677344888276238967708686478204144567903648898602832023986822307766599164931606556396324965787956232337282301952.000000 (data: 18.674759, reg: 59592921637677344888276238967708686478204144567903648898602832023986822307766599164931606556396324965787956232337282301952.000000)\n",
      "Iteration 200, Loss: 153883814633735020814121488803996746621982552936419507880559691639100580388175570336927870122634254414394972690173137876863554386904755454428976269194806579075171811223095651489345950032970530770389071155726878933117824791144722807895151869952.000000 (data: 18.674759, reg: 153883814633735020814121488803996746621982552936419507880559691639100580388175570336927870122634254414394972690173137876863554386904755454428976269194806579075171811223095651489345950032970530770389071155726878933117824791144722807895151869952.000000)\n",
      "Iteration 300, Loss: inf (data: 18.674759, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 18.674759, reg: inf)\n",
      "Iteration 500, Loss: inf (data: 18.674759, reg: inf)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.430563 (data: 1.090061, reg: 2.340502)\n",
      "Iteration 100, Loss: 1.078868 (data: 1.063277, reg: 0.015591)\n",
      "Iteration 200, Loss: 1.078777 (data: 1.063416, reg: 0.015362)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063462, reg: 0.015310)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063473, reg: 0.015299)\n",
      "Iteration 500, Loss: 1.078772 (data: 1.063476, reg: 0.015296)\n",
      "Iteration 600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.05, Iter: 1000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.316090 (data: 1.096852, reg: 0.219238)\n",
      "Iteration 100, Loss: 0.979677 (data: 0.899169, reg: 0.080509)\n",
      "Iteration 200, Loss: 0.979426 (data: 0.898036, reg: 0.081390)\n",
      "Iteration 300, Loss: 0.979351 (data: 0.897557, reg: 0.081794)\n",
      "Iteration 400, Loss: 0.979327 (data: 0.897297, reg: 0.082030)\n",
      "Iteration 500, Loss: 0.979319 (data: 0.897154, reg: 0.082165)\n",
      "Iteration 600, Loss: 0.979317 (data: 0.897075, reg: 0.082242)\n",
      "Iteration 700, Loss: 0.979316 (data: 0.897031, reg: 0.082285)\n",
      "Iteration 800, Loss: 0.979316 (data: 0.897006, reg: 0.082310)\n",
      "Iteration 900, Loss: 0.979316 (data: 0.896992, reg: 0.082324)\n",
      "LR: 0.05, Iter: 1000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.115754 (data: 1.092675, reg: 0.023079)\n",
      "Iteration 100, Loss: 0.748206 (data: 0.673137, reg: 0.075069)\n",
      "Iteration 200, Loss: 0.715920 (data: 0.606304, reg: 0.109615)\n",
      "Iteration 300, Loss: 0.707693 (data: 0.580720, reg: 0.126973)\n",
      "Iteration 400, Loss: 0.704817 (data: 0.568898, reg: 0.135919)\n",
      "Iteration 500, Loss: 0.703525 (data: 0.562878, reg: 0.140647)\n",
      "Iteration 600, Loss: 0.702803 (data: 0.559583, reg: 0.143220)\n",
      "Iteration 700, Loss: 0.702334 (data: 0.557656, reg: 0.144678)\n",
      "Iteration 800, Loss: 0.702003 (data: 0.556452, reg: 0.145551)\n",
      "Iteration 900, Loss: 0.701758 (data: 0.555648, reg: 0.146110)\n",
      "LR: 0.05, Iter: 1000, C: 10.0, Val Acc: 0.7584\n",
      "Iteration 0, Loss: 1.100570 (data: 1.098255, reg: 0.002314)\n",
      "Iteration 100, Loss: 0.639950 (data: 0.628354, reg: 0.011596)\n",
      "Iteration 200, Loss: 0.539728 (data: 0.517420, reg: 0.022308)\n",
      "Iteration 300, Loss: 0.486253 (data: 0.453830, reg: 0.032423)\n",
      "Iteration 400, Loss: 0.452171 (data: 0.410290, reg: 0.041881)\n",
      "Iteration 500, Loss: 0.428747 (data: 0.378101, reg: 0.050646)\n",
      "Iteration 600, Loss: 0.411923 (data: 0.353208, reg: 0.058714)\n",
      "Iteration 700, Loss: 0.399474 (data: 0.333369, reg: 0.066105)\n",
      "Iteration 800, Loss: 0.390061 (data: 0.317210, reg: 0.072852)\n",
      "Iteration 900, Loss: 0.382826 (data: 0.303831, reg: 0.078995)\n",
      "LR: 0.05, Iter: 1000, C: 100.0, Val Acc: 0.7248\n",
      "Iteration 0, Loss: 223.655778 (data: 1.106587, reg: 222.549191)\n",
      "Iteration 100, Loss: inf (data: 19.564033, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 23.963796 (data: 1.114051, reg: 22.849745)\n",
      "Iteration 100, Loss: 59074473936047140743843537811335720218612182922147613321888054785947225682603155020217318366442963135063342376379054292992.000000 (data: 21.342582, reg: 59074473936047140743843537811335720218612182922147613321888054785947225682603155020217318366442963135063342376379054292992.000000)\n",
      "Iteration 200, Loss: 152545053119405988202486018034587449260762950059324177869984731807628546618768004739833785199500325827256875395342187391837266064683278723182988260549804610840003236236757669142267072361664303228039060446498820275163476255580092369379883220992.000000 (data: 21.342582, reg: 152545053119405988202486018034587449260762950059324177869984731807628546618768004739833785199500325827256875395342187391837266064683278723182988260549804610840003236236757669142267072361664303228039060446498820275163476255580092369379883220992.000000)\n",
      "Iteration 300, Loss: inf (data: 21.342582, reg: inf)\n",
      "Iteration 400, Loss: inf (data: 21.342582, reg: inf)\n",
      "Iteration 500, Loss: inf (data: 21.342582, reg: inf)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.05, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.360943 (data: 1.100566, reg: 2.260377)\n",
      "Iteration 100, Loss: 1.078865 (data: 1.063279, reg: 0.015585)\n",
      "Iteration 200, Loss: 1.078777 (data: 1.063417, reg: 0.015360)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063462, reg: 0.015310)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063474, reg: 0.015299)\n",
      "Iteration 500, Loss: 1.078772 (data: 1.063476, reg: 0.015296)\n",
      "Iteration 600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1000, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1100, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1200, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1500, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.05, Iter: 2000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.337745 (data: 1.104236, reg: 0.233508)\n",
      "Iteration 100, Loss: 0.979700 (data: 0.899227, reg: 0.080473)\n",
      "Iteration 200, Loss: 0.979433 (data: 0.898071, reg: 0.081363)\n",
      "Iteration 300, Loss: 0.979353 (data: 0.897576, reg: 0.081777)\n",
      "Iteration 400, Loss: 0.979328 (data: 0.897307, reg: 0.082020)\n",
      "Iteration 500, Loss: 0.979320 (data: 0.897160, reg: 0.082160)\n",
      "Iteration 600, Loss: 0.979317 (data: 0.897078, reg: 0.082239)\n",
      "Iteration 700, Loss: 0.979316 (data: 0.897033, reg: 0.082284)\n",
      "Iteration 800, Loss: 0.979316 (data: 0.897007, reg: 0.082309)\n",
      "Iteration 900, Loss: 0.979316 (data: 0.896993, reg: 0.082323)\n",
      "Iteration 1000, Loss: 0.979316 (data: 0.896985, reg: 0.082331)\n",
      "Iteration 1100, Loss: 0.979316 (data: 0.896981, reg: 0.082335)\n",
      "Iteration 1200, Loss: 0.979316 (data: 0.896978, reg: 0.082338)\n",
      "Iteration 1300, Loss: 0.979316 (data: 0.896977, reg: 0.082339)\n",
      "Iteration 1400, Loss: 0.979316 (data: 0.896976, reg: 0.082340)\n",
      "Iteration 1500, Loss: 0.979316 (data: 0.896976, reg: 0.082340)\n",
      "Iteration 1600, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1700, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1800, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1900, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "LR: 0.05, Iter: 2000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.110819 (data: 1.088079, reg: 0.022740)\n",
      "Iteration 100, Loss: 0.747952 (data: 0.672923, reg: 0.075029)\n",
      "Iteration 200, Loss: 0.715778 (data: 0.606137, reg: 0.109641)\n",
      "Iteration 300, Loss: 0.707614 (data: 0.580587, reg: 0.127026)\n",
      "Iteration 400, Loss: 0.704772 (data: 0.568801, reg: 0.135971)\n",
      "Iteration 500, Loss: 0.703497 (data: 0.562809, reg: 0.140688)\n",
      "Iteration 600, Loss: 0.702784 (data: 0.559534, reg: 0.143249)\n",
      "Iteration 700, Loss: 0.702320 (data: 0.557621, reg: 0.144699)\n",
      "Iteration 800, Loss: 0.701992 (data: 0.556426, reg: 0.145566)\n",
      "Iteration 900, Loss: 0.701750 (data: 0.555628, reg: 0.146122)\n",
      "Iteration 1000, Loss: 0.701568 (data: 0.555061, reg: 0.146507)\n",
      "Iteration 1100, Loss: 0.701429 (data: 0.554636, reg: 0.146793)\n",
      "Iteration 1200, Loss: 0.701323 (data: 0.554304, reg: 0.147019)\n",
      "Iteration 1300, Loss: 0.701243 (data: 0.554036, reg: 0.147206)\n",
      "Iteration 1400, Loss: 0.701181 (data: 0.553815, reg: 0.147366)\n",
      "Iteration 1500, Loss: 0.701133 (data: 0.553629, reg: 0.147504)\n",
      "Iteration 1600, Loss: 0.701097 (data: 0.553471, reg: 0.147626)\n",
      "Iteration 1700, Loss: 0.701069 (data: 0.553336, reg: 0.147733)\n",
      "Iteration 1800, Loss: 0.701047 (data: 0.553220, reg: 0.147827)\n",
      "Iteration 1900, Loss: 0.701030 (data: 0.553119, reg: 0.147911)\n",
      "LR: 0.05, Iter: 2000, C: 10.0, Val Acc: 0.7517\n",
      "Iteration 0, Loss: 1.111158 (data: 1.108912, reg: 0.002246)\n",
      "Iteration 100, Loss: 0.639406 (data: 0.627865, reg: 0.011541)\n",
      "Iteration 200, Loss: 0.539299 (data: 0.517005, reg: 0.022294)\n",
      "Iteration 300, Loss: 0.486006 (data: 0.453589, reg: 0.032417)\n",
      "Iteration 400, Loss: 0.452026 (data: 0.410156, reg: 0.041870)\n",
      "Iteration 500, Loss: 0.428655 (data: 0.378027, reg: 0.050628)\n",
      "Iteration 600, Loss: 0.411858 (data: 0.353168, reg: 0.058690)\n",
      "Iteration 700, Loss: 0.399423 (data: 0.333348, reg: 0.066075)\n",
      "Iteration 800, Loss: 0.390018 (data: 0.317199, reg: 0.072819)\n",
      "Iteration 900, Loss: 0.382786 (data: 0.303825, reg: 0.078960)\n",
      "Iteration 1000, Loss: 0.377151 (data: 0.292608, reg: 0.084543)\n",
      "Iteration 1100, Loss: 0.372712 (data: 0.283101, reg: 0.089611)\n",
      "Iteration 1200, Loss: 0.369182 (data: 0.274976, reg: 0.094206)\n",
      "Iteration 1300, Loss: 0.366351 (data: 0.267982, reg: 0.098369)\n",
      "Iteration 1400, Loss: 0.364063 (data: 0.261925, reg: 0.102138)\n",
      "Iteration 1500, Loss: 0.362201 (data: 0.256652, reg: 0.105549)\n",
      "Iteration 1600, Loss: 0.360676 (data: 0.252040, reg: 0.108635)\n",
      "Iteration 1700, Loss: 0.359418 (data: 0.247991, reg: 0.111427)\n",
      "Iteration 1800, Loss: 0.358374 (data: 0.244422, reg: 0.113952)\n",
      "Iteration 1900, Loss: 0.357503 (data: 0.241267, reg: 0.116236)\n",
      "LR: 0.05, Iter: 2000, C: 100.0, Val Acc: 0.7114\n",
      "Iteration 0, Loss: 231.534804 (data: 1.099566, reg: 230.435237)\n",
      "Iteration 100, Loss: inf (data: 18.293642, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 500, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 24.605205 (data: 1.099318, reg: 23.505887)\n",
      "Iteration 100, Loss: 1658914702289365514109241813849557475584279858447236437038726955006569999913551026306697486111360037722674341260469827713709793596101345986803156621122901689529646306062896090673805576867151872.000000 (data: 18.992357, reg: 1658914702289365514109241813849557475584279858447236437038726955006569999913551026306697486111360037722674341260469827713709793596101345986803156621122901689529646306062896090673805576867151872.000000)\n",
      "Iteration 200, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 18.992357, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 500, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.408653 (data: 1.105616, reg: 2.303038)\n",
      "Iteration 100, Loss: 1.078777 (data: 1.063418, reg: 0.015360)\n",
      "Iteration 200, Loss: 1.078772 (data: 1.063474, reg: 0.015298)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.1, Iter: 500, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.328483 (data: 1.097665, reg: 0.230818)\n",
      "Iteration 100, Loss: 0.979433 (data: 0.898068, reg: 0.081365)\n",
      "Iteration 200, Loss: 0.979327 (data: 0.897305, reg: 0.082022)\n",
      "Iteration 300, Loss: 0.979317 (data: 0.897077, reg: 0.082240)\n",
      "Iteration 400, Loss: 0.979316 (data: 0.897007, reg: 0.082309)\n",
      "LR: 0.1, Iter: 500, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.124882 (data: 1.102212, reg: 0.022671)\n",
      "Iteration 100, Loss: 0.715815 (data: 0.606502, reg: 0.109314)\n",
      "Iteration 200, Loss: 0.704721 (data: 0.568836, reg: 0.135885)\n",
      "Iteration 300, Loss: 0.702749 (data: 0.559512, reg: 0.143237)\n",
      "Iteration 400, Loss: 0.701971 (data: 0.556397, reg: 0.145574)\n",
      "LR: 0.1, Iter: 500, C: 10.0, Val Acc: 0.7584\n",
      "Iteration 0, Loss: 1.095541 (data: 1.093301, reg: 0.002240)\n",
      "Iteration 100, Loss: 0.539556 (data: 0.517240, reg: 0.022316)\n",
      "Iteration 200, Loss: 0.452241 (data: 0.410387, reg: 0.041854)\n",
      "Iteration 300, Loss: 0.411985 (data: 0.353314, reg: 0.058671)\n",
      "Iteration 400, Loss: 0.390094 (data: 0.317289, reg: 0.072805)\n",
      "LR: 0.1, Iter: 500, C: 100.0, Val Acc: 0.7248\n",
      "Iteration 0, Loss: 235.426755 (data: 1.101075, reg: 234.325680)\n",
      "Iteration 100, Loss: inf (data: 18.674759, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 1000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 24.006390 (data: 1.105563, reg: 22.900827)\n",
      "Iteration 100, Loss: 1616636371098358496683379394405991813881816261004875539441382600262474272518613135745840884284993121807101533726870799862563331139757108246147857773195135993925175581322942236160258585904807936.000000 (data: 20.072190, reg: 1616636371098358496683379394405991813881816261004875539441382600262474272518613135745840884284993121807101533726870799862563331139757108246147857773195135993925175581322942236160258585904807936.000000)\n",
      "Iteration 200, Loss: inf (data: 20.072190, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 20.072190, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 1000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.442723 (data: 1.098782, reg: 2.343941)\n",
      "Iteration 100, Loss: 1.078777 (data: 1.063418, reg: 0.015359)\n",
      "Iteration 200, Loss: 1.078772 (data: 1.063474, reg: 0.015298)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 500, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.1, Iter: 1000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.333028 (data: 1.106005, reg: 0.227023)\n",
      "Iteration 100, Loss: 0.979430 (data: 0.898055, reg: 0.081375)\n",
      "Iteration 200, Loss: 0.979327 (data: 0.897302, reg: 0.082026)\n",
      "Iteration 300, Loss: 0.979317 (data: 0.897076, reg: 0.082241)\n",
      "Iteration 400, Loss: 0.979316 (data: 0.897006, reg: 0.082310)\n",
      "Iteration 500, Loss: 0.979316 (data: 0.896985, reg: 0.082331)\n",
      "Iteration 600, Loss: 0.979316 (data: 0.896978, reg: 0.082338)\n",
      "Iteration 700, Loss: 0.979316 (data: 0.896976, reg: 0.082340)\n",
      "Iteration 800, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 900, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "LR: 0.1, Iter: 1000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.126023 (data: 1.103734, reg: 0.022289)\n",
      "Iteration 100, Loss: 0.715857 (data: 0.606431, reg: 0.109426)\n",
      "Iteration 200, Loss: 0.704792 (data: 0.568887, reg: 0.135905)\n",
      "Iteration 300, Loss: 0.702795 (data: 0.559561, reg: 0.143234)\n",
      "Iteration 400, Loss: 0.701999 (data: 0.556436, reg: 0.145564)\n",
      "Iteration 500, Loss: 0.701572 (data: 0.555065, reg: 0.146507)\n",
      "Iteration 600, Loss: 0.701326 (data: 0.554307, reg: 0.147020)\n",
      "Iteration 700, Loss: 0.701182 (data: 0.553816, reg: 0.147366)\n",
      "Iteration 800, Loss: 0.701098 (data: 0.553472, reg: 0.147626)\n",
      "Iteration 900, Loss: 0.701048 (data: 0.553221, reg: 0.147827)\n",
      "LR: 0.1, Iter: 1000, C: 10.0, Val Acc: 0.7517\n",
      "Iteration 0, Loss: 1.097355 (data: 1.095051, reg: 0.002304)\n",
      "Iteration 100, Loss: 0.539452 (data: 0.517102, reg: 0.022350)\n",
      "Iteration 200, Loss: 0.452054 (data: 0.410145, reg: 0.041910)\n",
      "Iteration 300, Loss: 0.411839 (data: 0.353104, reg: 0.058736)\n",
      "Iteration 400, Loss: 0.389990 (data: 0.317119, reg: 0.072871)\n",
      "Iteration 500, Loss: 0.377127 (data: 0.292529, reg: 0.084598)\n",
      "Iteration 600, Loss: 0.369165 (data: 0.274904, reg: 0.094261)\n",
      "Iteration 700, Loss: 0.364053 (data: 0.261862, reg: 0.102191)\n",
      "Iteration 800, Loss: 0.360671 (data: 0.251986, reg: 0.108685)\n",
      "Iteration 900, Loss: 0.358373 (data: 0.244376, reg: 0.113997)\n",
      "LR: 0.1, Iter: 1000, C: 100.0, Val Acc: 0.7181\n",
      "Iteration 0, Loss: 224.263961 (data: 1.105765, reg: 223.158196)\n",
      "Iteration 100, Loss: inf (data: 20.580347, reg: inf)\n",
      "Iteration 200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 2000, C: 0.001, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 24.353176 (data: 1.094968, reg: 23.258209)\n",
      "Iteration 100, Loss: 1641145434417606215104790273925607990352825608147162697432236899945959325161184439182640630162818708295753638125221849616964183995047857835271908156843129159860179612422043122421226207690358784.000000 (data: 17.849004, reg: 1641145434417606215104790273925607990352825608147162697432236899945959325161184439182640630162818708295753638125221849616964183995047857835271908156843129159860179612422043122421226207690358784.000000)\n",
      "Iteration 200, Loss: inf (data: 17.849004, reg: inf)\n",
      "Iteration 300, Loss: inf (data: 17.849004, reg: inf)\n",
      "Iteration 400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 900, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1000, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1100, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1200, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1300, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1400, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1500, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1600, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1700, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1800, Loss: nan (data: nan, reg: nan)\n",
      "Iteration 1900, Loss: nan (data: nan, reg: nan)\n",
      "LR: 0.1, Iter: 2000, C: 0.01, Val Acc: 0.3691\n",
      "Iteration 0, Loss: 3.266674 (data: 1.099265, reg: 2.167408)\n",
      "Iteration 100, Loss: 1.078777 (data: 1.063417, reg: 0.015360)\n",
      "Iteration 200, Loss: 1.078772 (data: 1.063474, reg: 0.015298)\n",
      "Iteration 300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 500, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1000, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1100, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1200, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1300, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1400, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1500, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1600, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1700, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1800, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "Iteration 1900, Loss: 1.078772 (data: 1.063477, reg: 0.015295)\n",
      "LR: 0.1, Iter: 2000, C: 0.1, Val Acc: 0.3758\n",
      "Iteration 0, Loss: 1.334919 (data: 1.099695, reg: 0.235224)\n",
      "Iteration 100, Loss: 0.979434 (data: 0.898075, reg: 0.081359)\n",
      "Iteration 200, Loss: 0.979328 (data: 0.897308, reg: 0.082020)\n",
      "Iteration 300, Loss: 0.979317 (data: 0.897078, reg: 0.082239)\n",
      "Iteration 400, Loss: 0.979316 (data: 0.897007, reg: 0.082309)\n",
      "Iteration 500, Loss: 0.979316 (data: 0.896985, reg: 0.082331)\n",
      "Iteration 600, Loss: 0.979316 (data: 0.896978, reg: 0.082338)\n",
      "Iteration 700, Loss: 0.979316 (data: 0.896976, reg: 0.082340)\n",
      "Iteration 800, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 900, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1000, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1100, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1200, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1300, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1400, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1500, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1600, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1700, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1800, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "Iteration 1900, Loss: 0.979316 (data: 0.896975, reg: 0.082341)\n",
      "LR: 0.1, Iter: 2000, C: 1.0, Val Acc: 0.6644\n",
      "Iteration 0, Loss: 1.126101 (data: 1.103485, reg: 0.022616)\n",
      "Iteration 100, Loss: 0.715947 (data: 0.606635, reg: 0.109312)\n",
      "Iteration 200, Loss: 0.704776 (data: 0.568925, reg: 0.135852)\n",
      "Iteration 300, Loss: 0.702779 (data: 0.559563, reg: 0.143216)\n",
      "Iteration 400, Loss: 0.701988 (data: 0.556430, reg: 0.145559)\n",
      "Iteration 500, Loss: 0.701565 (data: 0.555059, reg: 0.146506)\n",
      "Iteration 600, Loss: 0.701322 (data: 0.554301, reg: 0.147020)\n",
      "Iteration 700, Loss: 0.701180 (data: 0.553812, reg: 0.147367)\n",
      "Iteration 800, Loss: 0.701096 (data: 0.553469, reg: 0.147627)\n",
      "Iteration 900, Loss: 0.701047 (data: 0.553218, reg: 0.147829)\n",
      "Iteration 1000, Loss: 0.701017 (data: 0.553031, reg: 0.147987)\n",
      "Iteration 1100, Loss: 0.701000 (data: 0.552890, reg: 0.148110)\n",
      "Iteration 1200, Loss: 0.700990 (data: 0.552784, reg: 0.148206)\n",
      "Iteration 1300, Loss: 0.700984 (data: 0.552703, reg: 0.148281)\n",
      "Iteration 1400, Loss: 0.700980 (data: 0.552641, reg: 0.148339)\n",
      "Iteration 1500, Loss: 0.700978 (data: 0.552594, reg: 0.148384)\n",
      "Iteration 1600, Loss: 0.700977 (data: 0.552558, reg: 0.148418)\n",
      "Iteration 1700, Loss: 0.700976 (data: 0.552531, reg: 0.148445)\n",
      "Iteration 1800, Loss: 0.700976 (data: 0.552510, reg: 0.148466)\n",
      "Iteration 1900, Loss: 0.700975 (data: 0.552494, reg: 0.148481)\n",
      "LR: 0.1, Iter: 2000, C: 10.0, Val Acc: 0.7584\n",
      "Iteration 0, Loss: 1.104881 (data: 1.102617, reg: 0.002265)\n",
      "Iteration 100, Loss: 0.539576 (data: 0.517300, reg: 0.022276)\n",
      "Iteration 200, Loss: 0.452039 (data: 0.410196, reg: 0.041843)\n",
      "Iteration 300, Loss: 0.411754 (data: 0.353069, reg: 0.058686)\n",
      "Iteration 400, Loss: 0.389885 (data: 0.317045, reg: 0.072840)\n",
      "Iteration 500, Loss: 0.377024 (data: 0.292441, reg: 0.084584)\n",
      "Iteration 600, Loss: 0.369072 (data: 0.274814, reg: 0.094259)\n",
      "Iteration 700, Loss: 0.363972 (data: 0.261775, reg: 0.102197)\n",
      "Iteration 800, Loss: 0.360601 (data: 0.251904, reg: 0.108697)\n",
      "Iteration 900, Loss: 0.358313 (data: 0.244301, reg: 0.114012)\n",
      "Iteration 1000, Loss: 0.356723 (data: 0.238363, reg: 0.118360)\n",
      "Iteration 1100, Loss: 0.355592 (data: 0.233673, reg: 0.121919)\n",
      "Iteration 1200, Loss: 0.354770 (data: 0.229935, reg: 0.124835)\n",
      "Iteration 1300, Loss: 0.354160 (data: 0.226932, reg: 0.127228)\n",
      "Iteration 1400, Loss: 0.353698 (data: 0.224502, reg: 0.129196)\n",
      "Iteration 1500, Loss: 0.353341 (data: 0.222524, reg: 0.130817)\n",
      "Iteration 1600, Loss: 0.353060 (data: 0.220904, reg: 0.132156)\n",
      "Iteration 1700, Loss: 0.352835 (data: 0.219572, reg: 0.133264)\n",
      "Iteration 1800, Loss: 0.352653 (data: 0.218469, reg: 0.134184)\n",
      "Iteration 1900, Loss: 0.352503 (data: 0.217553, reg: 0.134949)\n",
      "LR: 0.1, Iter: 2000, C: 100.0, Val Acc: 0.7181\n",
      "Best Validation Accuracy: 0.7584 with params: {'learning_rate': 0.05, 'num_iterations': 1000, 'C': 10.0}\n",
      "Iteration 0, Loss: 1.115796 (data: 1.093063, reg: 0.022733)\n",
      "Iteration 100, Loss: 0.747307 (data: 0.671926, reg: 0.075381)\n",
      "Iteration 200, Loss: 0.715581 (data: 0.605584, reg: 0.109997)\n",
      "Iteration 300, Loss: 0.707545 (data: 0.580302, reg: 0.127244)\n",
      "Iteration 400, Loss: 0.704738 (data: 0.568647, reg: 0.136092)\n",
      "Iteration 500, Loss: 0.703476 (data: 0.562720, reg: 0.140756)\n",
      "Iteration 600, Loss: 0.702769 (data: 0.559479, reg: 0.143290)\n",
      "Iteration 700, Loss: 0.702310 (data: 0.557584, reg: 0.144726)\n",
      "Iteration 800, Loss: 0.701984 (data: 0.556399, reg: 0.145585)\n",
      "Iteration 900, Loss: 0.701744 (data: 0.555608, reg: 0.146137)\n"
     ]
    }
   ],
   "source": [
    "# Tune all hyperparameters including C\n",
    "best_params = tune_hyperparameters(X_train, y_train_one, X_valid, y_valid_one)\n",
    "\n",
    "# Train with best parameters including C\n",
    "W, b = fit_softmax_regression(X_train.values, y_train_one.values, \n",
    "                             learning_rate=best_params['learning_rate'],\n",
    "                             num_iterations=best_params['num_iterations'],\n",
    "                             C=best_params['C'])\n",
    "val_pred = predict(X_valid.values, W, b)\n",
    "# test_pred = predict(X_test.values, W, b)\n",
    "\n",
    "val_acc = np.mean(val_pred == y_valid.values.ravel())\n",
    "# test_acc = np.mean(test_pred == y_test.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "fa710242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDATION SET ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.88        55\n",
      "           1       0.72      0.69      0.70        48\n",
      "           2       0.71      0.59      0.64        46\n",
      "\n",
      "    accuracy                           0.76       149\n",
      "   macro avg       0.75      0.75      0.74       149\n",
      "weighted avg       0.75      0.76      0.75       149\n",
      "\n",
      "Validation Accuracy: 0.7584\n"
     ]
    }
   ],
   "source": [
    "y_valid_true = y_valid.values.ravel()\n",
    "\n",
    "# y_test_true = y_test.values.ravel()\n",
    "\n",
    "print(\"=== VALIDATION SET ===\")\n",
    "print(classification_report(y_valid_true, val_pred))\n",
    "\n",
    "# print(\"=== TEST SET ===\")\n",
    "# print(classification_report(y_test_true, test_pred, digits=4))\n",
    "\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "# print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
